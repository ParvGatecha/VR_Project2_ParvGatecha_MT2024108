{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:07:18.490199Z",
     "iopub.status.busy": "2025-05-18T12:07:18.489728Z",
     "iopub.status.idle": "2025-05-18T12:08:41.412087Z",
     "shell.execute_reply": "2025-05-18T12:08:41.411163Z",
     "shell.execute_reply.started": "2025-05-18T12:07:18.490176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-18T12:08:41.414041Z",
     "iopub.status.busy": "2025-05-18T12:08:41.413776Z",
     "iopub.status.idle": "2025-05-18T12:09:09.796658Z",
     "shell.execute_reply": "2025-05-18T12:09:09.796095Z",
     "shell.execute_reply.started": "2025-05-18T12:08:41.414017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 12:08:55.380894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747570135.593043      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747570135.653163      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:09.797819Z",
     "iopub.status.busy": "2025-05-18T12:09:09.797358Z",
     "iopub.status.idle": "2025-05-18T12:09:09.802042Z",
     "shell.execute_reply": "2025-05-18T12:09:09.801283Z",
     "shell.execute_reply.started": "2025-05-18T12:09:09.797801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"dandelin/vilt-b32-finetuned-vqa\"\n",
    "CSV_PATH = \"/kaggle/input/amazon-vqa-dataset/merged.csv\"\n",
    "IMAGE_ROOT = \"/kaggle/input/amazon-vqa-images\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LENGTH = 40\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:09.804746Z",
     "iopub.status.busy": "2025-05-18T12:09:09.804189Z",
     "iopub.status.idle": "2025-05-18T12:09:14.318885Z",
     "shell.execute_reply": "2025-05-18T12:09:14.317997Z",
     "shell.execute_reply.started": "2025-05-18T12:09:09.804722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f295ac0b9ad04a91a0289de0cf6c4cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fae986c36247b89d3ce96beb455106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b95530a3c34c94835a1037b4af0a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1903f3d0ec48aeb0a3025a187c40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79385d4303a24f2db1199aa99c6d5e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbb536798ed45fba21dd73bc74a6923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e25637e46143b1bfcc722bbef1aa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = ViltProcessor.from_pretrained(MODEL_NAME)\n",
    "model = ViltForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "full_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df = full_df.iloc[:10000].reset_index(drop=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['answer'])\n",
    "num_classes = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:47.098871Z",
     "iopub.status.busy": "2025-05-18T12:09:47.098263Z",
     "iopub.status.idle": "2025-05-18T12:09:47.106932Z",
     "shell.execute_reply": "2025-05-18T12:09:47.106272Z",
     "shell.execute_reply.started": "2025-05-18T12:09:47.098843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, image_root):\n",
    "        self.data = dataframe\n",
    "        self.processor = processor\n",
    "        self.image_root = image_root\n",
    "        self.resize = transforms.Resize((384, 384))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        image = Image.open(os.path.join(self.image_root, item[\"image_path\"])).convert(\"RGB\")\n",
    "        image = self.resize(image)\n",
    "        encoding = self.processor(\n",
    "            images=image, \n",
    "            text=item[\"question\"], \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "        )\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(int(item[\"label\"])).long()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:48.691392Z",
     "iopub.status.busy": "2025-05-18T12:09:48.691079Z",
     "iopub.status.idle": "2025-05-18T12:09:48.700680Z",
     "shell.execute_reply": "2025-05-18T12:09:48.699951Z",
     "shell.execute_reply.started": "2025-05-18T12:09:48.691371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_dataset = VQADataset(train_df, processor, IMAGE_ROOT)\n",
    "val_dataset = VQADataset(val_df, processor, IMAGE_ROOT)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:51.945963Z",
     "iopub.status.busy": "2025-05-18T12:09:51.945371Z",
     "iopub.status.idle": "2025-05-18T12:09:52.691755Z",
     "shell.execute_reply": "2025-05-18T12:09:52.691143Z",
     "shell.execute_reply.started": "2025-05-18T12:09:51.945942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\",\"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:55.069497Z",
     "iopub.status.busy": "2025-05-18T12:09:55.069184Z",
     "iopub.status.idle": "2025-05-18T12:09:55.075709Z",
     "shell.execute_reply": "2025-05-18T12:09:55.075110Z",
     "shell.execute_reply.started": "2025-05-18T12:09:55.069477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:09:57.823509Z",
     "iopub.status.busy": "2025-05-18T12:09:57.823203Z",
     "iopub.status.idle": "2025-05-18T12:09:57.829973Z",
     "shell.execute_reply": "2025-05-18T12:09:57.829339Z",
     "shell.execute_reply.started": "2025-05-18T12:09:57.823489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(train_df, processor, IMAGE_ROOT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = VQADataset(val_df, processor, IMAGE_ROOT)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:10:00.741155Z",
     "iopub.status.busy": "2025-05-18T12:10:00.740495Z",
     "iopub.status.idle": "2025-05-18T12:58:59.992870Z",
     "shell.execute_reply": "2025-05-18T12:58:59.992204Z",
     "shell.execute_reply.started": "2025-05-18T12:10:00.741134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] Loss: 16.5075\n",
      "[Batch 10] Loss: 12.9167\n",
      "[Batch 20] Loss: 12.7982\n",
      "[Batch 30] Loss: 13.0660\n",
      "[Batch 40] Loss: 13.1376\n",
      "[Batch 50] Loss: 12.8834\n",
      "[Batch 60] Loss: 11.5431\n",
      "[Batch 70] Loss: 15.0252\n",
      "[Batch 80] Loss: 14.2385\n",
      "[Batch 90] Loss: 12.3309\n",
      "[Batch 100] Loss: 10.7889\n",
      "[Batch 110] Loss: 10.6791\n",
      "[Batch 120] Loss: 14.5354\n",
      "[Batch 130] Loss: 13.4280\n",
      "[Batch 140] Loss: 15.3726\n",
      "[Batch 150] Loss: 12.2334\n",
      "[Batch 160] Loss: 12.0656\n",
      "[Batch 170] Loss: 11.7608\n",
      "[Batch 180] Loss: 12.9401\n",
      "[Batch 190] Loss: 12.0693\n",
      "[Batch 200] Loss: 9.2784\n",
      "[Batch 210] Loss: 10.1894\n",
      "[Batch 220] Loss: 10.4256\n",
      "[Batch 230] Loss: 8.2377\n",
      "[Batch 240] Loss: 8.5830\n",
      "[Batch 250] Loss: 6.5307\n",
      "[Batch 260] Loss: 7.8323\n",
      "[Batch 270] Loss: 9.8828\n",
      "[Batch 280] Loss: 7.9106\n",
      "[Batch 290] Loss: 8.8729\n",
      "[Batch 300] Loss: 9.7937\n",
      "[Batch 310] Loss: 9.1855\n",
      "[Batch 320] Loss: 8.5487\n",
      "[Batch 330] Loss: 7.2863\n",
      "[Batch 340] Loss: 6.3611\n",
      "[Batch 350] Loss: 10.3810\n",
      "[Batch 360] Loss: 4.0772\n",
      "[Batch 370] Loss: 7.5670\n",
      "[Batch 380] Loss: 8.8568\n",
      "[Batch 390] Loss: 6.7889\n",
      "[Batch 400] Loss: 7.0393\n",
      "[Batch 410] Loss: 4.0814\n",
      "[Batch 420] Loss: 6.8221\n",
      "[Batch 430] Loss: 5.8029\n",
      "[Batch 440] Loss: 7.4083\n",
      "[Batch 450] Loss: 7.6877\n",
      "[Batch 460] Loss: 5.6325\n",
      "[Batch 470] Loss: 5.5201\n",
      "[Batch 480] Loss: 5.7675\n",
      "[Batch 490] Loss: 6.2540\n",
      "[Batch 500] Loss: 4.6582\n",
      "[Batch 510] Loss: 6.6670\n",
      "[Batch 520] Loss: 4.8614\n",
      "[Batch 530] Loss: 7.1223\n",
      "[Batch 540] Loss: 7.8538\n",
      "[Batch 550] Loss: 6.7896\n",
      "[Batch 560] Loss: 3.9003\n",
      "[Batch 570] Loss: 8.0867\n",
      "[Batch 580] Loss: 7.1229\n",
      "[Batch 590] Loss: 6.2460\n",
      "[Batch 600] Loss: 7.5063\n",
      "[Batch 610] Loss: 6.0142\n",
      "[Batch 620] Loss: 7.2315\n",
      "[Batch 630] Loss: 4.9447\n",
      "[Batch 640] Loss: 8.5477\n",
      "[Batch 650] Loss: 9.3793\n",
      "[Batch 660] Loss: 3.8665\n",
      "[Batch 670] Loss: 4.7835\n",
      "[Batch 680] Loss: 7.1569\n",
      "[Batch 690] Loss: 9.3919\n",
      "[Batch 700] Loss: 6.7920\n",
      "[Batch 710] Loss: 5.4785\n",
      "[Batch 720] Loss: 6.6413\n",
      "[Batch 730] Loss: 5.2218\n",
      "[Batch 740] Loss: 7.3085\n",
      "[Batch 750] Loss: 7.0437\n",
      "[Batch 760] Loss: 5.4927\n",
      "[Batch 770] Loss: 5.8797\n",
      "[Batch 780] Loss: 3.0279\n",
      "[Batch 790] Loss: 4.6150\n",
      "[Batch 800] Loss: 5.1813\n",
      "[Batch 810] Loss: 7.7798\n",
      "[Batch 820] Loss: 11.1494\n",
      "[Batch 830] Loss: 5.9327\n",
      "[Batch 840] Loss: 7.6018\n",
      "[Batch 850] Loss: 3.9941\n",
      "[Batch 860] Loss: 5.8005\n",
      "[Batch 870] Loss: 3.6906\n",
      "[Batch 880] Loss: 5.3115\n",
      "[Batch 890] Loss: 7.6160\n",
      "[Batch 900] Loss: 6.7208\n",
      "[Batch 910] Loss: 7.6552\n",
      "[Batch 920] Loss: 6.3634\n",
      "[Batch 930] Loss: 3.5819\n",
      "[Batch 940] Loss: 3.8688\n",
      "[Batch 950] Loss: 6.7010\n",
      "[Batch 960] Loss: 5.3821\n",
      "[Batch 970] Loss: 5.9779\n",
      "[Batch 980] Loss: 3.1691\n",
      "[Batch 990] Loss: 4.4081\n",
      "[Batch 1000] Loss: 6.1251\n",
      "[Batch 1010] Loss: 5.9317\n",
      "[Batch 1020] Loss: 3.6120\n",
      "[Batch 1030] Loss: 7.2551\n",
      "[Batch 1040] Loss: 7.7061\n",
      "[Batch 1050] Loss: 8.0372\n",
      "[Batch 1060] Loss: 1.6946\n",
      "[Batch 1070] Loss: 4.7900\n",
      "[Batch 1080] Loss: 7.9423\n",
      "[Batch 1090] Loss: 6.0804\n",
      "[Batch 1100] Loss: 6.6430\n",
      "[Batch 1110] Loss: 4.0778\n",
      "[Batch 1120] Loss: 6.7178\n",
      "[Batch 1130] Loss: 4.9229\n",
      "[Batch 1140] Loss: 6.2033\n",
      "[Batch 1150] Loss: 6.9892\n",
      "[Batch 1160] Loss: 8.0922\n",
      "[Batch 1170] Loss: 3.7512\n",
      "[Batch 1180] Loss: 6.5848\n",
      "[Batch 1190] Loss: 7.9167\n",
      "[Batch 1200] Loss: 6.3035\n",
      "[Batch 1210] Loss: 7.3999\n",
      "[Batch 1220] Loss: 4.4681\n",
      "[Batch 1230] Loss: 2.4516\n",
      "[Batch 1240] Loss: 7.1088\n",
      "[Batch 1250] Loss: 5.4519\n",
      "[Batch 1260] Loss: 7.9840\n",
      "[Batch 1270] Loss: 5.0028\n",
      "[Batch 1280] Loss: 7.1480\n",
      "[Batch 1290] Loss: 5.5343\n",
      "[Batch 1300] Loss: 5.5155\n",
      "[Batch 1310] Loss: 3.7913\n",
      "[Batch 1320] Loss: 6.4057\n",
      "[Batch 1330] Loss: 5.4323\n",
      "[Batch 1340] Loss: 6.6998\n",
      "[Batch 1350] Loss: 6.8881\n",
      "[Batch 1360] Loss: 4.9003\n",
      "[Batch 1370] Loss: 4.5556\n",
      "[Batch 1380] Loss: 7.7781\n",
      "[Batch 1390] Loss: 3.6559\n",
      "[Batch 1400] Loss: 5.6263\n",
      "[Batch 1410] Loss: 6.7524\n",
      "[Batch 1420] Loss: 3.7401\n",
      "[Batch 1430] Loss: 3.9716\n",
      "[Batch 1440] Loss: 4.1181\n",
      "[Batch 1450] Loss: 5.0572\n",
      "[Batch 1460] Loss: 8.5554\n",
      "[Batch 1470] Loss: 7.7367\n",
      "[Batch 1480] Loss: 6.2704\n",
      "[Batch 1490] Loss: 5.9318\n",
      "[Batch 1500] Loss: 4.0535\n",
      "[Batch 1510] Loss: 3.1592\n",
      "[Batch 1520] Loss: 2.1856\n",
      "[Batch 1530] Loss: 5.2280\n",
      "[Batch 1540] Loss: 8.0541\n",
      "[Batch 1550] Loss: 6.4540\n",
      "[Batch 1560] Loss: 6.5595\n",
      "[Batch 1570] Loss: 4.9964\n",
      "[Batch 1580] Loss: 9.0128\n",
      "[Batch 1590] Loss: 5.9818\n",
      "[Batch 1600] Loss: 4.1930\n",
      "[Batch 1610] Loss: 5.7477\n",
      "[Batch 1620] Loss: 7.2107\n",
      "[Batch 1630] Loss: 3.6962\n",
      "[Batch 1640] Loss: 5.9465\n",
      "[Batch 1650] Loss: 6.8236\n",
      "[Batch 1660] Loss: 4.8120\n",
      "[Batch 1670] Loss: 3.6264\n",
      "[Batch 1680] Loss: 7.0697\n",
      "[Batch 1690] Loss: 2.4227\n",
      "[Batch 1700] Loss: 3.1407\n",
      "[Batch 1710] Loss: 6.8100\n",
      "[Batch 1720] Loss: 7.9107\n",
      "[Batch 1730] Loss: 5.8763\n",
      "[Batch 1740] Loss: 5.5086\n",
      "[Batch 1750] Loss: 8.3639\n",
      "[Batch 1760] Loss: 2.9901\n",
      "[Batch 1770] Loss: 7.6512\n",
      "[Batch 1780] Loss: 1.9710\n",
      "[Batch 1790] Loss: 3.3707\n",
      "[Batch 1800] Loss: 7.0838\n",
      "[Batch 1810] Loss: 3.3004\n",
      "[Batch 1820] Loss: 6.9897\n",
      "[Batch 1830] Loss: 5.9824\n",
      "[Batch 1840] Loss: 2.5667\n",
      "[Batch 1850] Loss: 6.4832\n",
      "[Batch 1860] Loss: 2.8170\n",
      "[Batch 1870] Loss: 6.0258\n",
      "[Batch 1880] Loss: 4.4306\n",
      "[Batch 1890] Loss: 3.4711\n",
      "[Batch 1900] Loss: 6.8942\n",
      "[Batch 1910] Loss: 5.3687\n",
      "[Batch 1920] Loss: 8.3340\n",
      "[Batch 1930] Loss: 4.7047\n",
      "[Batch 1940] Loss: 3.2545\n",
      "[Batch 1950] Loss: 6.1662\n",
      "[Batch 1960] Loss: 2.8512\n",
      "[Batch 1970] Loss: 5.2969\n",
      "[Batch 1980] Loss: 2.6906\n",
      "[Batch 1990] Loss: 2.5488\n",
      "[Batch 2000] Loss: 2.2513\n",
      "[Batch 2010] Loss: 8.2913\n",
      "[Batch 2020] Loss: 6.6265\n",
      "[Batch 2030] Loss: 1.8747\n",
      "[Batch 2040] Loss: 3.1155\n",
      "[Batch 2050] Loss: 6.9078\n",
      "[Batch 2060] Loss: 5.9474\n",
      "[Batch 2070] Loss: 5.4114\n",
      "[Batch 2080] Loss: 2.3182\n",
      "[Batch 2090] Loss: 5.3710\n",
      "[Batch 2100] Loss: 2.2146\n",
      "[Batch 2110] Loss: 2.7785\n",
      "[Batch 2120] Loss: 4.2390\n",
      "[Batch 2130] Loss: 4.8428\n",
      "[Batch 2140] Loss: 7.2893\n",
      "[Batch 2150] Loss: 0.7889\n",
      "[Batch 2160] Loss: 4.7646\n",
      "[Batch 2170] Loss: 4.6898\n",
      "[Batch 2180] Loss: 6.2603\n",
      "[Batch 2190] Loss: 7.3063\n",
      "[Batch 2200] Loss: 5.3215\n",
      "[Batch 2210] Loss: 2.5555\n",
      "[Batch 2220] Loss: 4.7818\n",
      "[Batch 2230] Loss: 4.7450\n",
      "[Batch 2240] Loss: 8.0121\n",
      "Epoch 1/10 | Loss: 14219.7720\n",
      "[Batch 0] Loss: 6.5524\n",
      "[Batch 10] Loss: 2.0323\n",
      "[Batch 20] Loss: 7.5212\n",
      "[Batch 30] Loss: 4.6541\n",
      "[Batch 40] Loss: 6.0784\n",
      "[Batch 50] Loss: 2.6180\n",
      "[Batch 60] Loss: 2.6644\n",
      "[Batch 70] Loss: 6.2469\n",
      "[Batch 80] Loss: 6.4246\n",
      "[Batch 90] Loss: 3.8173\n",
      "[Batch 100] Loss: 4.3972\n",
      "[Batch 110] Loss: 3.7027\n",
      "[Batch 120] Loss: 1.9069\n",
      "[Batch 130] Loss: 3.1204\n",
      "[Batch 140] Loss: 4.5038\n",
      "[Batch 150] Loss: 9.2591\n",
      "[Batch 160] Loss: 2.8513\n",
      "[Batch 170] Loss: 5.7028\n",
      "[Batch 180] Loss: 4.8811\n",
      "[Batch 190] Loss: 2.5943\n",
      "[Batch 200] Loss: 5.2763\n",
      "[Batch 210] Loss: 3.1045\n",
      "[Batch 220] Loss: 2.6229\n",
      "[Batch 230] Loss: 7.4328\n",
      "[Batch 240] Loss: 5.1662\n",
      "[Batch 250] Loss: 5.1514\n",
      "[Batch 260] Loss: 5.1999\n",
      "[Batch 270] Loss: 7.2201\n",
      "[Batch 280] Loss: 1.9644\n",
      "[Batch 290] Loss: 5.0790\n",
      "[Batch 300] Loss: 2.5490\n",
      "[Batch 310] Loss: 2.4141\n",
      "[Batch 320] Loss: 6.2236\n",
      "[Batch 330] Loss: 6.3064\n",
      "[Batch 340] Loss: 2.6396\n",
      "[Batch 350] Loss: 4.1319\n",
      "[Batch 360] Loss: 5.2343\n",
      "[Batch 370] Loss: 4.1261\n",
      "[Batch 380] Loss: 3.7427\n",
      "[Batch 390] Loss: 4.9315\n",
      "[Batch 400] Loss: 4.0167\n",
      "[Batch 410] Loss: 5.3719\n",
      "[Batch 420] Loss: 7.3483\n",
      "[Batch 430] Loss: 4.3271\n",
      "[Batch 440] Loss: 5.5438\n",
      "[Batch 450] Loss: 3.0287\n",
      "[Batch 460] Loss: 6.2232\n",
      "[Batch 470] Loss: 2.8970\n",
      "[Batch 480] Loss: 4.7134\n",
      "[Batch 490] Loss: 4.5467\n",
      "[Batch 500] Loss: 3.9561\n",
      "[Batch 510] Loss: 5.1532\n",
      "[Batch 520] Loss: 2.7491\n",
      "[Batch 530] Loss: 6.1664\n",
      "[Batch 540] Loss: 3.1278\n",
      "[Batch 550] Loss: 1.1116\n",
      "[Batch 560] Loss: 3.9356\n",
      "[Batch 570] Loss: 3.3955\n",
      "[Batch 580] Loss: 3.4518\n",
      "[Batch 590] Loss: 4.9609\n",
      "[Batch 600] Loss: 0.7939\n",
      "[Batch 610] Loss: 2.7277\n",
      "[Batch 620] Loss: 5.2246\n",
      "[Batch 630] Loss: 8.7611\n",
      "[Batch 640] Loss: 4.8671\n",
      "[Batch 650] Loss: 5.1333\n",
      "[Batch 660] Loss: 4.0069\n",
      "[Batch 670] Loss: 6.2461\n",
      "[Batch 680] Loss: 4.2246\n",
      "[Batch 690] Loss: 2.8280\n",
      "[Batch 700] Loss: 5.7662\n",
      "[Batch 710] Loss: 3.0028\n",
      "[Batch 720] Loss: 3.3690\n",
      "[Batch 730] Loss: 5.5088\n",
      "[Batch 740] Loss: 2.4991\n",
      "[Batch 750] Loss: 3.3437\n",
      "[Batch 760] Loss: 4.3467\n",
      "[Batch 770] Loss: 6.0447\n",
      "[Batch 780] Loss: 5.2762\n",
      "[Batch 790] Loss: 4.9460\n",
      "[Batch 800] Loss: 3.1911\n",
      "[Batch 810] Loss: 6.6998\n",
      "[Batch 820] Loss: 1.0552\n",
      "[Batch 830] Loss: 0.9185\n",
      "[Batch 840] Loss: 8.0582\n",
      "[Batch 850] Loss: 6.2728\n",
      "[Batch 860] Loss: 5.3363\n",
      "[Batch 870] Loss: 2.7743\n",
      "[Batch 880] Loss: 8.3392\n",
      "[Batch 890] Loss: 3.5554\n",
      "[Batch 900] Loss: 4.2158\n",
      "[Batch 910] Loss: 5.7698\n",
      "[Batch 920] Loss: 2.5868\n",
      "[Batch 930] Loss: 1.7410\n",
      "[Batch 940] Loss: 5.3384\n",
      "[Batch 950] Loss: 8.1613\n",
      "[Batch 960] Loss: 4.5741\n",
      "[Batch 970] Loss: 7.3043\n",
      "[Batch 980] Loss: 2.6726\n",
      "[Batch 990] Loss: 5.0535\n",
      "[Batch 1000] Loss: 5.5459\n",
      "[Batch 1010] Loss: 1.4166\n",
      "[Batch 1020] Loss: 4.8910\n",
      "[Batch 1030] Loss: 3.1565\n",
      "[Batch 1040] Loss: 1.5307\n",
      "[Batch 1050] Loss: 7.1204\n",
      "[Batch 1060] Loss: 4.4896\n",
      "[Batch 1070] Loss: 3.6715\n",
      "[Batch 1080] Loss: 5.6995\n",
      "[Batch 1090] Loss: 3.6447\n",
      "[Batch 1100] Loss: 2.2139\n",
      "[Batch 1110] Loss: 2.6319\n",
      "[Batch 1120] Loss: 6.8142\n",
      "[Batch 1130] Loss: 3.4278\n",
      "[Batch 1140] Loss: 5.1719\n",
      "[Batch 1150] Loss: 2.7926\n",
      "[Batch 1160] Loss: 5.1299\n",
      "[Batch 1170] Loss: 4.4957\n",
      "[Batch 1180] Loss: 6.1150\n",
      "[Batch 1190] Loss: 6.3714\n",
      "[Batch 1200] Loss: 3.7636\n",
      "[Batch 1210] Loss: 3.3795\n",
      "[Batch 1220] Loss: 3.4069\n",
      "[Batch 1230] Loss: 5.5504\n",
      "[Batch 1240] Loss: 2.4255\n",
      "[Batch 1250] Loss: 4.7837\n",
      "[Batch 1260] Loss: 4.8230\n",
      "[Batch 1270] Loss: 3.2089\n",
      "[Batch 1280] Loss: 5.4306\n",
      "[Batch 1290] Loss: 2.8688\n",
      "[Batch 1300] Loss: 4.2232\n",
      "[Batch 1310] Loss: 4.8289\n",
      "[Batch 1320] Loss: 3.9954\n",
      "[Batch 1330] Loss: 3.2080\n",
      "[Batch 1340] Loss: 5.3092\n",
      "[Batch 1350] Loss: 5.2151\n",
      "[Batch 1360] Loss: 1.1679\n",
      "[Batch 1370] Loss: 5.6988\n",
      "[Batch 1380] Loss: 3.9585\n",
      "[Batch 1390] Loss: 4.3829\n",
      "[Batch 1400] Loss: 2.7873\n",
      "[Batch 1410] Loss: 5.3225\n",
      "[Batch 1420] Loss: 5.8436\n",
      "[Batch 1430] Loss: 2.3528\n",
      "[Batch 1440] Loss: 3.3660\n",
      "[Batch 1450] Loss: 3.7167\n",
      "[Batch 1460] Loss: 4.6410\n",
      "[Batch 1470] Loss: 2.3568\n",
      "[Batch 1480] Loss: 2.3590\n",
      "[Batch 1490] Loss: 6.6119\n",
      "[Batch 1500] Loss: 4.8544\n",
      "[Batch 1510] Loss: 1.2765\n",
      "[Batch 1520] Loss: 1.4132\n",
      "[Batch 1530] Loss: 4.9186\n",
      "[Batch 1540] Loss: 4.0936\n",
      "[Batch 1550] Loss: 4.8673\n",
      "[Batch 1560] Loss: 3.5567\n",
      "[Batch 1570] Loss: 4.6328\n",
      "[Batch 1580] Loss: 4.6587\n",
      "[Batch 1590] Loss: 3.5333\n",
      "[Batch 1600] Loss: 4.9384\n",
      "[Batch 1610] Loss: 7.5566\n",
      "[Batch 1620] Loss: 6.3715\n",
      "[Batch 1630] Loss: 5.0949\n",
      "[Batch 1640] Loss: 3.8481\n",
      "[Batch 1650] Loss: 3.3892\n",
      "[Batch 1660] Loss: 2.7982\n",
      "[Batch 1670] Loss: 2.6200\n",
      "[Batch 1680] Loss: 6.2932\n",
      "[Batch 1690] Loss: 3.8959\n",
      "[Batch 1700] Loss: 2.0965\n",
      "[Batch 1710] Loss: 3.8755\n",
      "[Batch 1720] Loss: 2.3651\n",
      "[Batch 1730] Loss: 4.2613\n",
      "[Batch 1740] Loss: 5.3324\n",
      "[Batch 1750] Loss: 5.1636\n",
      "[Batch 1760] Loss: 6.2390\n",
      "[Batch 1770] Loss: 4.5387\n",
      "[Batch 1780] Loss: 2.5264\n",
      "[Batch 1790] Loss: 4.7361\n",
      "[Batch 1800] Loss: 2.5931\n",
      "[Batch 1810] Loss: 3.3584\n",
      "[Batch 1820] Loss: 4.0020\n",
      "[Batch 1830] Loss: 5.5655\n",
      "[Batch 1840] Loss: 2.3107\n",
      "[Batch 1850] Loss: 2.3643\n",
      "[Batch 1860] Loss: 3.6569\n",
      "[Batch 1870] Loss: 2.8914\n",
      "[Batch 1880] Loss: 5.3924\n",
      "[Batch 1890] Loss: 2.8885\n",
      "[Batch 1900] Loss: 3.3473\n",
      "[Batch 1910] Loss: 2.6634\n",
      "[Batch 1920] Loss: 1.4695\n",
      "[Batch 1930] Loss: 7.5314\n",
      "[Batch 1940] Loss: 2.3312\n",
      "[Batch 1950] Loss: 7.0563\n",
      "[Batch 1960] Loss: 7.3109\n",
      "[Batch 1970] Loss: 7.0573\n",
      "[Batch 1980] Loss: 5.0938\n",
      "[Batch 1990] Loss: 1.5678\n",
      "[Batch 2000] Loss: 1.6965\n",
      "[Batch 2010] Loss: 3.2284\n",
      "[Batch 2020] Loss: 4.1681\n",
      "[Batch 2030] Loss: 4.4768\n",
      "[Batch 2040] Loss: 3.8871\n",
      "[Batch 2050] Loss: 3.1617\n",
      "[Batch 2060] Loss: 4.6303\n",
      "[Batch 2070] Loss: 2.7523\n",
      "[Batch 2080] Loss: 7.3764\n",
      "[Batch 2090] Loss: 6.4449\n",
      "[Batch 2100] Loss: 5.5953\n",
      "[Batch 2110] Loss: 3.0215\n",
      "[Batch 2120] Loss: 4.6771\n",
      "[Batch 2130] Loss: 7.6892\n",
      "[Batch 2140] Loss: 6.4209\n",
      "[Batch 2150] Loss: 7.2648\n",
      "[Batch 2160] Loss: 4.6881\n",
      "[Batch 2170] Loss: 5.5045\n",
      "[Batch 2180] Loss: 3.4742\n",
      "[Batch 2190] Loss: 2.3674\n",
      "[Batch 2200] Loss: 4.5528\n",
      "[Batch 2210] Loss: 2.8848\n",
      "[Batch 2220] Loss: 0.7237\n",
      "[Batch 2230] Loss: 4.7773\n",
      "[Batch 2240] Loss: 2.3909\n",
      "Epoch 2/10 | Loss: 9504.1651\n",
      "[Batch 0] Loss: 2.9673\n",
      "[Batch 10] Loss: 6.8056\n",
      "[Batch 20] Loss: 5.5554\n",
      "[Batch 30] Loss: 4.3927\n",
      "[Batch 40] Loss: 1.0817\n",
      "[Batch 50] Loss: 5.7767\n",
      "[Batch 60] Loss: 4.5405\n",
      "[Batch 70] Loss: 4.2027\n",
      "[Batch 80] Loss: 4.6324\n",
      "[Batch 90] Loss: 6.6800\n",
      "[Batch 100] Loss: 3.2862\n",
      "[Batch 110] Loss: 1.8792\n",
      "[Batch 120] Loss: 4.7991\n",
      "[Batch 130] Loss: 1.2265\n",
      "[Batch 140] Loss: 2.8384\n",
      "[Batch 150] Loss: 6.0112\n",
      "[Batch 160] Loss: 5.6842\n",
      "[Batch 170] Loss: 5.7290\n",
      "[Batch 180] Loss: 3.6843\n",
      "[Batch 190] Loss: 1.3470\n",
      "[Batch 200] Loss: 7.1095\n",
      "[Batch 210] Loss: 5.0167\n",
      "[Batch 220] Loss: 2.6020\n",
      "[Batch 230] Loss: 2.7108\n",
      "[Batch 240] Loss: 1.6408\n",
      "[Batch 250] Loss: 3.8390\n",
      "[Batch 260] Loss: 4.2123\n",
      "[Batch 270] Loss: 6.2498\n",
      "[Batch 280] Loss: 4.2343\n",
      "[Batch 290] Loss: 4.7425\n",
      "[Batch 300] Loss: 2.1920\n",
      "[Batch 310] Loss: 0.7008\n",
      "[Batch 320] Loss: 2.7379\n",
      "[Batch 330] Loss: 0.6844\n",
      "[Batch 340] Loss: 4.0475\n",
      "[Batch 350] Loss: 8.6350\n",
      "[Batch 360] Loss: 3.7567\n",
      "[Batch 370] Loss: 3.2960\n",
      "[Batch 380] Loss: 6.1828\n",
      "[Batch 390] Loss: 1.5750\n",
      "[Batch 400] Loss: 4.0636\n",
      "[Batch 410] Loss: 2.3925\n",
      "[Batch 420] Loss: 3.5115\n",
      "[Batch 430] Loss: 6.6754\n",
      "[Batch 440] Loss: 2.9637\n",
      "[Batch 450] Loss: 6.0669\n",
      "[Batch 460] Loss: 1.9706\n",
      "[Batch 470] Loss: 3.5823\n",
      "[Batch 480] Loss: 6.0152\n",
      "[Batch 490] Loss: 3.6802\n",
      "[Batch 500] Loss: 4.6150\n",
      "[Batch 510] Loss: 4.4133\n",
      "[Batch 520] Loss: 3.4074\n",
      "[Batch 530] Loss: 5.2896\n",
      "[Batch 540] Loss: 2.6923\n",
      "[Batch 550] Loss: 4.5410\n",
      "[Batch 560] Loss: 1.5085\n",
      "[Batch 570] Loss: 0.5089\n",
      "[Batch 580] Loss: 2.9940\n",
      "[Batch 590] Loss: 6.1879\n",
      "[Batch 600] Loss: 5.1629\n",
      "[Batch 610] Loss: 0.8003\n",
      "[Batch 620] Loss: 0.4562\n",
      "[Batch 630] Loss: 2.4821\n",
      "[Batch 640] Loss: 5.9400\n",
      "[Batch 650] Loss: 5.3573\n",
      "[Batch 660] Loss: 4.2687\n",
      "[Batch 670] Loss: 2.8915\n",
      "[Batch 680] Loss: 4.4267\n",
      "[Batch 690] Loss: 1.0961\n",
      "[Batch 700] Loss: 6.4020\n",
      "[Batch 710] Loss: 2.4064\n",
      "[Batch 720] Loss: 4.9477\n",
      "[Batch 730] Loss: 1.9794\n",
      "[Batch 740] Loss: 3.2213\n",
      "[Batch 750] Loss: 3.1426\n",
      "[Batch 760] Loss: 4.4916\n",
      "[Batch 770] Loss: 2.0040\n",
      "[Batch 780] Loss: 1.2801\n",
      "[Batch 790] Loss: 4.2024\n",
      "[Batch 800] Loss: 5.6306\n",
      "[Batch 810] Loss: 2.8725\n",
      "[Batch 820] Loss: 3.8847\n",
      "[Batch 830] Loss: 3.7813\n",
      "[Batch 840] Loss: 6.2832\n",
      "[Batch 850] Loss: 4.0937\n",
      "[Batch 860] Loss: 2.7483\n",
      "[Batch 870] Loss: 5.2317\n",
      "[Batch 880] Loss: 4.1377\n",
      "[Batch 890] Loss: 2.6549\n",
      "[Batch 900] Loss: 0.6201\n",
      "[Batch 910] Loss: 5.1489\n",
      "[Batch 920] Loss: 3.8160\n",
      "[Batch 930] Loss: 3.7986\n",
      "[Batch 940] Loss: 5.2864\n",
      "[Batch 950] Loss: 4.2569\n",
      "[Batch 960] Loss: 4.4632\n",
      "[Batch 970] Loss: 3.9475\n",
      "[Batch 980] Loss: 5.4034\n",
      "[Batch 990] Loss: 3.4401\n",
      "[Batch 1000] Loss: 5.8658\n",
      "[Batch 1010] Loss: 5.0916\n",
      "[Batch 1020] Loss: 3.4345\n",
      "[Batch 1030] Loss: 3.3707\n",
      "[Batch 1040] Loss: 3.7248\n",
      "[Batch 1050] Loss: 5.4021\n",
      "[Batch 1060] Loss: 2.1732\n",
      "[Batch 1070] Loss: 5.3829\n",
      "[Batch 1080] Loss: 4.1356\n",
      "[Batch 1090] Loss: 2.8496\n",
      "[Batch 1100] Loss: 2.8914\n",
      "[Batch 1110] Loss: 3.6463\n",
      "[Batch 1120] Loss: 4.1062\n",
      "[Batch 1130] Loss: 5.0386\n",
      "[Batch 1140] Loss: 1.6763\n",
      "[Batch 1150] Loss: 1.9105\n",
      "[Batch 1160] Loss: 2.5937\n",
      "[Batch 1170] Loss: 2.7039\n",
      "[Batch 1180] Loss: 4.4138\n",
      "[Batch 1190] Loss: 3.8293\n",
      "[Batch 1200] Loss: 0.4300\n",
      "[Batch 1210] Loss: 0.8322\n",
      "[Batch 1220] Loss: 2.8011\n",
      "[Batch 1230] Loss: 2.2693\n",
      "[Batch 1240] Loss: 5.4365\n",
      "[Batch 1250] Loss: 2.1500\n",
      "[Batch 1260] Loss: 2.5213\n",
      "[Batch 1270] Loss: 1.5852\n",
      "[Batch 1280] Loss: 1.3055\n",
      "[Batch 1290] Loss: 0.8157\n",
      "[Batch 1300] Loss: 5.2729\n",
      "[Batch 1310] Loss: 5.7305\n",
      "[Batch 1320] Loss: 2.1565\n",
      "[Batch 1330] Loss: 1.5468\n",
      "[Batch 1340] Loss: 3.5859\n",
      "[Batch 1350] Loss: 5.1837\n",
      "[Batch 1360] Loss: 1.8978\n",
      "[Batch 1370] Loss: 2.5663\n",
      "[Batch 1380] Loss: 2.8707\n",
      "[Batch 1390] Loss: 3.3694\n",
      "[Batch 1400] Loss: 3.6620\n",
      "[Batch 1410] Loss: 2.5309\n",
      "[Batch 1420] Loss: 2.3906\n",
      "[Batch 1430] Loss: 4.1136\n",
      "[Batch 1440] Loss: 2.8191\n",
      "[Batch 1450] Loss: 3.9311\n",
      "[Batch 1460] Loss: 4.0215\n",
      "[Batch 1470] Loss: 1.9398\n",
      "[Batch 1480] Loss: 2.3725\n",
      "[Batch 1490] Loss: 3.4053\n",
      "[Batch 1500] Loss: 2.9629\n",
      "[Batch 1510] Loss: 5.4016\n",
      "[Batch 1520] Loss: 6.2698\n",
      "[Batch 1530] Loss: 2.0937\n",
      "[Batch 1540] Loss: 3.5575\n",
      "[Batch 1550] Loss: 4.8338\n",
      "[Batch 1560] Loss: 2.4343\n",
      "[Batch 1570] Loss: 5.4836\n",
      "[Batch 1580] Loss: 2.8483\n",
      "[Batch 1590] Loss: 4.5527\n",
      "[Batch 1600] Loss: 3.2128\n",
      "[Batch 1610] Loss: 5.4039\n",
      "[Batch 1620] Loss: 4.1385\n",
      "[Batch 1630] Loss: 6.2544\n",
      "[Batch 1640] Loss: 3.4019\n",
      "[Batch 1650] Loss: 5.5270\n",
      "[Batch 1660] Loss: 2.2805\n",
      "[Batch 1670] Loss: 2.5543\n",
      "[Batch 1680] Loss: 3.9659\n",
      "[Batch 1690] Loss: 3.8434\n",
      "[Batch 1700] Loss: 7.8111\n",
      "[Batch 1710] Loss: 3.7596\n",
      "[Batch 1720] Loss: 3.3135\n",
      "[Batch 1730] Loss: 5.3527\n",
      "[Batch 1740] Loss: 4.4324\n",
      "[Batch 1750] Loss: 3.4683\n",
      "[Batch 1760] Loss: 3.1669\n",
      "[Batch 1770] Loss: 4.5986\n",
      "[Batch 1780] Loss: 4.9468\n",
      "[Batch 1790] Loss: 2.6031\n",
      "[Batch 1800] Loss: 3.3337\n",
      "[Batch 1810] Loss: 6.8857\n",
      "[Batch 1820] Loss: 2.6603\n",
      "[Batch 1830] Loss: 4.8114\n",
      "[Batch 1840] Loss: 2.9713\n",
      "[Batch 1850] Loss: 4.0090\n",
      "[Batch 1860] Loss: 5.2508\n",
      "[Batch 1870] Loss: 3.7459\n",
      "[Batch 1880] Loss: 3.6869\n",
      "[Batch 1890] Loss: 4.5996\n",
      "[Batch 1900] Loss: 3.1776\n",
      "[Batch 1910] Loss: 2.3623\n",
      "[Batch 1920] Loss: 3.2878\n",
      "[Batch 1930] Loss: 1.3968\n",
      "[Batch 1940] Loss: 4.3192\n",
      "[Batch 1950] Loss: 5.7668\n",
      "[Batch 1960] Loss: 4.1010\n",
      "[Batch 1970] Loss: 2.7519\n",
      "[Batch 1980] Loss: 4.7535\n",
      "[Batch 1990] Loss: 2.1916\n",
      "[Batch 2000] Loss: 1.3663\n",
      "[Batch 2010] Loss: 3.4317\n",
      "[Batch 2020] Loss: 2.5250\n",
      "[Batch 2030] Loss: 0.7024\n",
      "[Batch 2040] Loss: 0.9520\n",
      "[Batch 2050] Loss: 2.6058\n",
      "[Batch 2060] Loss: 3.5513\n",
      "[Batch 2070] Loss: 4.4918\n",
      "[Batch 2080] Loss: 2.7942\n",
      "[Batch 2090] Loss: 5.0643\n",
      "[Batch 2100] Loss: 5.9680\n",
      "[Batch 2110] Loss: 4.7035\n",
      "[Batch 2120] Loss: 0.3801\n",
      "[Batch 2130] Loss: 2.6577\n",
      "[Batch 2140] Loss: 1.7467\n",
      "[Batch 2150] Loss: 1.3626\n",
      "[Batch 2160] Loss: 5.2830\n",
      "[Batch 2170] Loss: 2.5872\n",
      "[Batch 2180] Loss: 6.5616\n",
      "[Batch 2190] Loss: 4.5166\n",
      "[Batch 2200] Loss: 6.8469\n",
      "[Batch 2210] Loss: 4.6266\n",
      "[Batch 2220] Loss: 1.0898\n",
      "[Batch 2230] Loss: 5.3594\n",
      "[Batch 2240] Loss: 1.5131\n",
      "Epoch 3/10 | Loss: 8179.9934\n",
      "[Batch 0] Loss: 2.7623\n",
      "[Batch 10] Loss: 2.4573\n",
      "[Batch 20] Loss: 3.3316\n",
      "[Batch 30] Loss: 2.1300\n",
      "[Batch 40] Loss: 1.4171\n",
      "[Batch 50] Loss: 1.5752\n",
      "[Batch 60] Loss: 2.7799\n",
      "[Batch 70] Loss: 3.7018\n",
      "[Batch 80] Loss: 3.5149\n",
      "[Batch 90] Loss: 2.6819\n",
      "[Batch 100] Loss: 3.8691\n",
      "[Batch 110] Loss: 3.5746\n",
      "[Batch 120] Loss: 3.2147\n",
      "[Batch 130] Loss: 3.0296\n",
      "[Batch 140] Loss: 3.6570\n",
      "[Batch 150] Loss: 2.0408\n",
      "[Batch 160] Loss: 2.5782\n",
      "[Batch 170] Loss: 3.3690\n",
      "[Batch 180] Loss: 3.0559\n",
      "[Batch 190] Loss: 3.1313\n",
      "[Batch 200] Loss: 3.9628\n",
      "[Batch 210] Loss: 1.7069\n",
      "[Batch 220] Loss: 1.2827\n",
      "[Batch 230] Loss: 1.9477\n",
      "[Batch 240] Loss: 2.2112\n",
      "[Batch 250] Loss: 3.4468\n",
      "[Batch 260] Loss: 3.5066\n",
      "[Batch 270] Loss: 2.7938\n",
      "[Batch 280] Loss: 0.9232\n",
      "[Batch 290] Loss: 3.1800\n",
      "[Batch 300] Loss: 2.2540\n",
      "[Batch 310] Loss: 3.8475\n",
      "[Batch 320] Loss: 4.1858\n",
      "[Batch 330] Loss: 1.6647\n",
      "[Batch 340] Loss: 4.5143\n",
      "[Batch 350] Loss: 2.2750\n",
      "[Batch 360] Loss: 4.6306\n",
      "[Batch 370] Loss: 4.8809\n",
      "[Batch 380] Loss: 2.8239\n",
      "[Batch 390] Loss: 2.9456\n",
      "[Batch 400] Loss: 2.5609\n",
      "[Batch 410] Loss: 0.8596\n",
      "[Batch 420] Loss: 2.1971\n",
      "[Batch 430] Loss: 2.1615\n",
      "[Batch 440] Loss: 2.7721\n",
      "[Batch 450] Loss: 2.6468\n",
      "[Batch 460] Loss: 3.8913\n",
      "[Batch 470] Loss: 4.3665\n",
      "[Batch 480] Loss: 4.4351\n",
      "[Batch 490] Loss: 4.9071\n",
      "[Batch 500] Loss: 3.7865\n",
      "[Batch 510] Loss: 3.6084\n",
      "[Batch 520] Loss: 2.5027\n",
      "[Batch 530] Loss: 2.3484\n",
      "[Batch 540] Loss: 2.6308\n",
      "[Batch 550] Loss: 3.8580\n",
      "[Batch 560] Loss: 3.5124\n",
      "[Batch 570] Loss: 2.6625\n",
      "[Batch 580] Loss: 2.1520\n",
      "[Batch 590] Loss: 2.6256\n",
      "[Batch 600] Loss: 6.3127\n",
      "[Batch 610] Loss: 1.4933\n",
      "[Batch 620] Loss: 2.3466\n",
      "[Batch 630] Loss: 2.9002\n",
      "[Batch 640] Loss: 3.5537\n",
      "[Batch 650] Loss: 3.0504\n",
      "[Batch 660] Loss: 4.3531\n",
      "[Batch 670] Loss: 1.2259\n",
      "[Batch 680] Loss: 4.2132\n",
      "[Batch 690] Loss: 4.2554\n",
      "[Batch 700] Loss: 4.0838\n",
      "[Batch 710] Loss: 3.0452\n",
      "[Batch 720] Loss: 2.3065\n",
      "[Batch 730] Loss: 5.4025\n",
      "[Batch 740] Loss: 2.5833\n",
      "[Batch 750] Loss: 2.6185\n",
      "[Batch 760] Loss: 5.3914\n",
      "[Batch 770] Loss: 4.9752\n",
      "[Batch 780] Loss: 4.2499\n",
      "[Batch 790] Loss: 3.7822\n",
      "[Batch 800] Loss: 2.4732\n",
      "[Batch 810] Loss: 4.2037\n",
      "[Batch 820] Loss: 2.0240\n",
      "[Batch 830] Loss: 2.9062\n",
      "[Batch 840] Loss: 4.4445\n",
      "[Batch 850] Loss: 5.2954\n",
      "[Batch 860] Loss: 2.8690\n",
      "[Batch 870] Loss: 1.8337\n",
      "[Batch 880] Loss: 2.9261\n",
      "[Batch 890] Loss: 4.7840\n",
      "[Batch 900] Loss: 3.2925\n",
      "[Batch 910] Loss: 5.7422\n",
      "[Batch 920] Loss: 3.8156\n",
      "[Batch 930] Loss: 3.6647\n",
      "[Batch 940] Loss: 3.5258\n",
      "[Batch 950] Loss: 4.6925\n",
      "[Batch 960] Loss: 2.6525\n",
      "[Batch 970] Loss: 2.8179\n",
      "[Batch 980] Loss: 5.1791\n",
      "[Batch 990] Loss: 4.7684\n",
      "[Batch 1000] Loss: 2.3475\n",
      "[Batch 1010] Loss: 2.3805\n",
      "[Batch 1020] Loss: 4.9297\n",
      "[Batch 1030] Loss: 2.2977\n",
      "[Batch 1040] Loss: 4.2575\n",
      "[Batch 1050] Loss: 3.9400\n",
      "[Batch 1060] Loss: 2.4925\n",
      "[Batch 1070] Loss: 4.1374\n",
      "[Batch 1080] Loss: 3.0931\n",
      "[Batch 1090] Loss: 4.8937\n",
      "[Batch 1100] Loss: 3.0653\n",
      "[Batch 1110] Loss: 5.9490\n",
      "[Batch 1120] Loss: 0.8771\n",
      "[Batch 1130] Loss: 1.0660\n",
      "[Batch 1140] Loss: 2.8467\n",
      "[Batch 1150] Loss: 3.0864\n",
      "[Batch 1160] Loss: 1.5815\n",
      "[Batch 1170] Loss: 3.8854\n",
      "[Batch 1180] Loss: 3.8686\n",
      "[Batch 1190] Loss: 1.7623\n",
      "[Batch 1200] Loss: 4.4961\n",
      "[Batch 1210] Loss: 3.6956\n",
      "[Batch 1220] Loss: 2.9573\n",
      "[Batch 1230] Loss: 3.4754\n",
      "[Batch 1240] Loss: 4.5223\n",
      "[Batch 1250] Loss: 2.6492\n",
      "[Batch 1260] Loss: 0.9676\n",
      "[Batch 1270] Loss: 3.2470\n",
      "[Batch 1280] Loss: 2.7505\n",
      "[Batch 1290] Loss: 2.8646\n",
      "[Batch 1300] Loss: 3.4482\n",
      "[Batch 1310] Loss: 1.4079\n",
      "[Batch 1320] Loss: 3.2796\n",
      "[Batch 1330] Loss: 0.9782\n",
      "[Batch 1340] Loss: 3.5247\n",
      "[Batch 1350] Loss: 4.8052\n",
      "[Batch 1360] Loss: 2.7540\n",
      "[Batch 1370] Loss: 4.0940\n",
      "[Batch 1380] Loss: 3.1705\n",
      "[Batch 1390] Loss: 3.0068\n",
      "[Batch 1400] Loss: 2.6043\n",
      "[Batch 1410] Loss: 1.5372\n",
      "[Batch 1420] Loss: 4.0014\n",
      "[Batch 1430] Loss: 3.8885\n",
      "[Batch 1440] Loss: 2.6129\n",
      "[Batch 1450] Loss: 5.6328\n",
      "[Batch 1460] Loss: 4.4307\n",
      "[Batch 1470] Loss: 2.7713\n",
      "[Batch 1480] Loss: 3.5485\n",
      "[Batch 1490] Loss: 3.8049\n",
      "[Batch 1500] Loss: 0.8167\n",
      "[Batch 1510] Loss: 2.7949\n",
      "[Batch 1520] Loss: 0.6545\n",
      "[Batch 1530] Loss: 4.6825\n",
      "[Batch 1540] Loss: 4.6042\n",
      "[Batch 1550] Loss: 4.8292\n",
      "[Batch 1560] Loss: 2.8394\n",
      "[Batch 1570] Loss: 3.5903\n",
      "[Batch 1580] Loss: 2.9240\n",
      "[Batch 1590] Loss: 2.2753\n",
      "[Batch 1600] Loss: 4.1626\n",
      "[Batch 1610] Loss: 5.0482\n",
      "[Batch 1620] Loss: 1.8050\n",
      "[Batch 1630] Loss: 3.5774\n",
      "[Batch 1640] Loss: 3.1795\n",
      "[Batch 1650] Loss: 3.1763\n",
      "[Batch 1660] Loss: 3.0875\n",
      "[Batch 1670] Loss: 6.3978\n",
      "[Batch 1680] Loss: 6.7711\n",
      "[Batch 1690] Loss: 3.6052\n",
      "[Batch 1700] Loss: 1.2507\n",
      "[Batch 1710] Loss: 1.1234\n",
      "[Batch 1720] Loss: 3.4312\n",
      "[Batch 1730] Loss: 4.3081\n",
      "[Batch 1740] Loss: 4.8529\n",
      "[Batch 1750] Loss: 4.8748\n",
      "[Batch 1760] Loss: 2.7052\n",
      "[Batch 1770] Loss: 2.2600\n",
      "[Batch 1780] Loss: 1.7679\n",
      "[Batch 1790] Loss: 1.7841\n",
      "[Batch 1800] Loss: 4.2028\n",
      "[Batch 1810] Loss: 5.0084\n",
      "[Batch 1820] Loss: 3.1928\n",
      "[Batch 1830] Loss: 4.9803\n",
      "[Batch 1840] Loss: 3.1925\n",
      "[Batch 1850] Loss: 2.2925\n",
      "[Batch 1860] Loss: 3.9968\n",
      "[Batch 1870] Loss: 3.1511\n",
      "[Batch 1880] Loss: 4.1708\n",
      "[Batch 1890] Loss: 1.9376\n",
      "[Batch 1900] Loss: 3.9343\n",
      "[Batch 1910] Loss: 3.2649\n",
      "[Batch 1920] Loss: 6.0008\n",
      "[Batch 1930] Loss: 0.8680\n",
      "[Batch 1940] Loss: 2.4332\n",
      "[Batch 1950] Loss: 2.7465\n",
      "[Batch 1960] Loss: 0.8552\n",
      "[Batch 1970] Loss: 0.9465\n",
      "[Batch 1980] Loss: 3.1282\n",
      "[Batch 1990] Loss: 3.6234\n",
      "[Batch 2000] Loss: 3.2934\n",
      "[Batch 2010] Loss: 4.4866\n",
      "[Batch 2020] Loss: 3.4822\n",
      "[Batch 2030] Loss: 3.3471\n",
      "[Batch 2040] Loss: 2.9048\n",
      "[Batch 2050] Loss: 1.4099\n",
      "[Batch 2060] Loss: 3.2649\n",
      "[Batch 2070] Loss: 3.3252\n",
      "[Batch 2080] Loss: 4.2032\n",
      "[Batch 2090] Loss: 3.1104\n",
      "[Batch 2100] Loss: 2.9962\n",
      "[Batch 2110] Loss: 1.3944\n",
      "[Batch 2120] Loss: 5.4081\n",
      "[Batch 2130] Loss: 3.8149\n",
      "[Batch 2140] Loss: 3.2822\n",
      "[Batch 2150] Loss: 4.7013\n",
      "[Batch 2160] Loss: 3.7854\n",
      "[Batch 2170] Loss: 4.4238\n",
      "[Batch 2180] Loss: 2.1834\n",
      "[Batch 2190] Loss: 3.1596\n",
      "[Batch 2200] Loss: 5.4343\n",
      "[Batch 2210] Loss: 3.7487\n",
      "[Batch 2220] Loss: 1.4874\n",
      "[Batch 2230] Loss: 2.3746\n",
      "[Batch 2240] Loss: 2.6880\n",
      "Epoch 4/10 | Loss: 7268.8838\n",
      "[Batch 0] Loss: 3.7698\n",
      "[Batch 10] Loss: 6.5105\n",
      "[Batch 20] Loss: 2.1280\n",
      "[Batch 30] Loss: 0.9975\n",
      "[Batch 40] Loss: 2.4698\n",
      "[Batch 50] Loss: 2.0694\n",
      "[Batch 60] Loss: 4.5259\n",
      "[Batch 70] Loss: 2.7592\n",
      "[Batch 80] Loss: 2.0461\n",
      "[Batch 90] Loss: 5.0972\n",
      "[Batch 100] Loss: 1.3239\n",
      "[Batch 110] Loss: 5.1194\n",
      "[Batch 120] Loss: 5.7120\n",
      "[Batch 130] Loss: 2.9514\n",
      "[Batch 140] Loss: 4.0674\n",
      "[Batch 150] Loss: 1.6547\n",
      "[Batch 160] Loss: 1.8069\n",
      "[Batch 170] Loss: 2.2987\n",
      "[Batch 180] Loss: 2.7209\n",
      "[Batch 190] Loss: 2.3347\n",
      "[Batch 200] Loss: 3.7929\n",
      "[Batch 210] Loss: 5.3686\n",
      "[Batch 220] Loss: 6.7867\n",
      "[Batch 230] Loss: 2.5786\n",
      "[Batch 240] Loss: 2.8719\n",
      "[Batch 250] Loss: 0.8029\n",
      "[Batch 260] Loss: 1.1084\n",
      "[Batch 270] Loss: 2.5069\n",
      "[Batch 280] Loss: 3.4380\n",
      "[Batch 290] Loss: 3.5168\n",
      "[Batch 300] Loss: 4.1904\n",
      "[Batch 310] Loss: 2.3217\n",
      "[Batch 320] Loss: 0.7139\n",
      "[Batch 330] Loss: 3.4642\n",
      "[Batch 340] Loss: 2.6838\n",
      "[Batch 350] Loss: 3.8672\n",
      "[Batch 360] Loss: 3.0007\n",
      "[Batch 370] Loss: 1.9249\n",
      "[Batch 380] Loss: 1.6814\n",
      "[Batch 390] Loss: 2.9283\n",
      "[Batch 400] Loss: 1.2957\n",
      "[Batch 410] Loss: 1.0592\n",
      "[Batch 420] Loss: 2.6101\n",
      "[Batch 430] Loss: 3.8822\n",
      "[Batch 440] Loss: 2.4204\n",
      "[Batch 450] Loss: 2.5198\n",
      "[Batch 460] Loss: 2.2429\n",
      "[Batch 470] Loss: 4.3782\n",
      "[Batch 480] Loss: 4.0589\n",
      "[Batch 490] Loss: 3.4874\n",
      "[Batch 500] Loss: 4.4142\n",
      "[Batch 510] Loss: 2.0987\n",
      "[Batch 520] Loss: 0.8726\n",
      "[Batch 530] Loss: 3.2513\n",
      "[Batch 540] Loss: 4.7483\n",
      "[Batch 550] Loss: 4.2764\n",
      "[Batch 560] Loss: 4.2610\n",
      "[Batch 570] Loss: 3.4088\n",
      "[Batch 580] Loss: 2.9664\n",
      "[Batch 590] Loss: 1.9591\n",
      "[Batch 600] Loss: 5.3087\n",
      "[Batch 610] Loss: 2.6057\n",
      "[Batch 620] Loss: 4.1093\n",
      "[Batch 630] Loss: 1.7804\n",
      "[Batch 640] Loss: 1.4226\n",
      "[Batch 650] Loss: 1.5752\n",
      "[Batch 660] Loss: 4.2754\n",
      "[Batch 670] Loss: 5.2721\n",
      "[Batch 680] Loss: 2.5639\n",
      "[Batch 690] Loss: 1.6877\n",
      "[Batch 700] Loss: 2.8944\n",
      "[Batch 710] Loss: 2.1300\n",
      "[Batch 720] Loss: 2.6447\n",
      "[Batch 730] Loss: 1.9172\n",
      "[Batch 740] Loss: 4.2638\n",
      "[Batch 750] Loss: 1.9097\n",
      "[Batch 760] Loss: 1.0844\n",
      "[Batch 770] Loss: 2.5121\n",
      "[Batch 780] Loss: 1.0038\n",
      "[Batch 790] Loss: 3.2670\n",
      "[Batch 800] Loss: 1.8142\n",
      "[Batch 810] Loss: 2.2770\n",
      "[Batch 820] Loss: 3.9429\n",
      "[Batch 830] Loss: 6.8783\n",
      "[Batch 840] Loss: 2.8216\n",
      "[Batch 850] Loss: 1.9126\n",
      "[Batch 860] Loss: 3.0079\n",
      "[Batch 870] Loss: 5.3115\n",
      "[Batch 880] Loss: 1.2713\n",
      "[Batch 890] Loss: 1.5299\n",
      "[Batch 900] Loss: 2.4093\n",
      "[Batch 910] Loss: 3.4489\n",
      "[Batch 920] Loss: 4.5807\n",
      "[Batch 930] Loss: 2.5153\n",
      "[Batch 940] Loss: 1.9630\n",
      "[Batch 950] Loss: 4.1436\n",
      "[Batch 960] Loss: 2.2939\n",
      "[Batch 970] Loss: 3.7575\n",
      "[Batch 980] Loss: 5.5202\n",
      "[Batch 990] Loss: 3.6971\n",
      "[Batch 1000] Loss: 2.9207\n",
      "[Batch 1010] Loss: 5.0069\n",
      "[Batch 1020] Loss: 3.3639\n",
      "[Batch 1030] Loss: 2.3525\n",
      "[Batch 1040] Loss: 2.6479\n",
      "[Batch 1050] Loss: 1.6486\n",
      "[Batch 1060] Loss: 1.4445\n",
      "[Batch 1070] Loss: 2.1737\n",
      "[Batch 1080] Loss: 1.3843\n",
      "[Batch 1090] Loss: 4.3698\n",
      "[Batch 1100] Loss: 5.9242\n",
      "[Batch 1110] Loss: 5.4724\n",
      "[Batch 1120] Loss: 0.3152\n",
      "[Batch 1130] Loss: 4.6864\n",
      "[Batch 1140] Loss: 3.8709\n",
      "[Batch 1150] Loss: 5.2095\n",
      "[Batch 1160] Loss: 2.5119\n",
      "[Batch 1170] Loss: 4.6591\n",
      "[Batch 1180] Loss: 3.2812\n",
      "[Batch 1190] Loss: 2.0585\n",
      "[Batch 1200] Loss: 3.0413\n",
      "[Batch 1210] Loss: 2.7945\n",
      "[Batch 1220] Loss: 3.8089\n",
      "[Batch 1230] Loss: 1.5408\n",
      "[Batch 1240] Loss: 2.4174\n",
      "[Batch 1250] Loss: 2.3712\n",
      "[Batch 1260] Loss: 3.1372\n",
      "[Batch 1270] Loss: 0.8978\n",
      "[Batch 1280] Loss: 1.6788\n",
      "[Batch 1290] Loss: 3.2384\n",
      "[Batch 1300] Loss: 4.0762\n",
      "[Batch 1310] Loss: 1.9763\n",
      "[Batch 1320] Loss: 3.2539\n",
      "[Batch 1330] Loss: 1.8939\n",
      "[Batch 1340] Loss: 2.6476\n",
      "[Batch 1350] Loss: 2.3176\n",
      "[Batch 1360] Loss: 5.5179\n",
      "[Batch 1370] Loss: 3.3139\n",
      "[Batch 1380] Loss: 1.5271\n",
      "[Batch 1390] Loss: 3.3949\n",
      "[Batch 1400] Loss: 3.7960\n",
      "[Batch 1410] Loss: 3.5609\n",
      "[Batch 1420] Loss: 3.4946\n",
      "[Batch 1430] Loss: 2.9624\n",
      "[Batch 1440] Loss: 3.3567\n",
      "[Batch 1450] Loss: 4.2634\n",
      "[Batch 1460] Loss: 2.1514\n",
      "[Batch 1470] Loss: 3.2852\n",
      "[Batch 1480] Loss: 4.3620\n",
      "[Batch 1490] Loss: 2.5386\n",
      "[Batch 1500] Loss: 0.9737\n",
      "[Batch 1510] Loss: 2.1961\n",
      "[Batch 1520] Loss: 2.1775\n",
      "[Batch 1530] Loss: 3.3495\n",
      "[Batch 1540] Loss: 3.4462\n",
      "[Batch 1550] Loss: 4.2969\n",
      "[Batch 1560] Loss: 1.6904\n",
      "[Batch 1570] Loss: 4.2761\n",
      "[Batch 1580] Loss: 2.0500\n",
      "[Batch 1590] Loss: 1.2127\n",
      "[Batch 1600] Loss: 4.4913\n",
      "[Batch 1610] Loss: 5.8923\n",
      "[Batch 1620] Loss: 3.4691\n",
      "[Batch 1630] Loss: 4.3649\n",
      "[Batch 1640] Loss: 0.5894\n",
      "[Batch 1650] Loss: 1.6725\n",
      "[Batch 1660] Loss: 3.6896\n",
      "[Batch 1670] Loss: 3.9399\n",
      "[Batch 1680] Loss: 3.7173\n",
      "[Batch 1690] Loss: 3.6295\n",
      "[Batch 1700] Loss: 2.4962\n",
      "[Batch 1710] Loss: 3.4673\n",
      "[Batch 1720] Loss: 5.8855\n",
      "[Batch 1730] Loss: 4.7118\n",
      "[Batch 1740] Loss: 3.8435\n",
      "[Batch 1750] Loss: 3.2253\n",
      "[Batch 1760] Loss: 1.9891\n",
      "[Batch 1770] Loss: 2.1883\n",
      "[Batch 1780] Loss: 0.8589\n",
      "[Batch 1790] Loss: 2.8716\n",
      "[Batch 1800] Loss: 2.2625\n",
      "[Batch 1810] Loss: 6.0729\n",
      "[Batch 1820] Loss: 3.0559\n",
      "[Batch 1830] Loss: 2.9118\n",
      "[Batch 1840] Loss: 2.6465\n",
      "[Batch 1850] Loss: 1.8746\n",
      "[Batch 1860] Loss: 5.3025\n",
      "[Batch 1870] Loss: 1.2388\n",
      "[Batch 1880] Loss: 1.8084\n",
      "[Batch 1890] Loss: 2.5409\n",
      "[Batch 1900] Loss: 2.0440\n",
      "[Batch 1910] Loss: 1.6938\n",
      "[Batch 1920] Loss: 2.2687\n",
      "[Batch 1930] Loss: 2.2268\n",
      "[Batch 1940] Loss: 4.0506\n",
      "[Batch 1950] Loss: 5.7922\n",
      "[Batch 1960] Loss: 2.2248\n",
      "[Batch 1970] Loss: 4.9677\n",
      "[Batch 1980] Loss: 3.5006\n",
      "[Batch 1990] Loss: 2.6674\n",
      "[Batch 2000] Loss: 3.3077\n",
      "[Batch 2010] Loss: 6.1196\n",
      "[Batch 2020] Loss: 4.1484\n",
      "[Batch 2030] Loss: 3.1905\n",
      "[Batch 2040] Loss: 2.0419\n",
      "[Batch 2050] Loss: 4.3400\n",
      "[Batch 2060] Loss: 3.1075\n",
      "[Batch 2070] Loss: 1.8370\n",
      "[Batch 2080] Loss: 1.8765\n",
      "[Batch 2090] Loss: 3.1104\n",
      "[Batch 2100] Loss: 2.7144\n",
      "[Batch 2110] Loss: 3.4380\n",
      "[Batch 2120] Loss: 1.0694\n",
      "[Batch 2130] Loss: 5.5027\n",
      "[Batch 2140] Loss: 2.2954\n",
      "[Batch 2150] Loss: 0.7868\n",
      "[Batch 2160] Loss: 1.6827\n",
      "[Batch 2170] Loss: 4.1875\n",
      "[Batch 2180] Loss: 2.8321\n",
      "[Batch 2190] Loss: 3.1414\n",
      "[Batch 2200] Loss: 3.4599\n",
      "[Batch 2210] Loss: 1.6395\n",
      "[Batch 2220] Loss: 1.2713\n",
      "[Batch 2230] Loss: 4.4696\n",
      "[Batch 2240] Loss: 2.8263\n",
      "Epoch 5/10 | Loss: 6523.9798\n",
      "[Batch 0] Loss: 3.2875\n",
      "[Batch 10] Loss: 4.7049\n",
      "[Batch 20] Loss: 1.9975\n",
      "[Batch 30] Loss: 2.2775\n",
      "[Batch 40] Loss: 2.9602\n",
      "[Batch 50] Loss: 0.9551\n",
      "[Batch 60] Loss: 0.5117\n",
      "[Batch 70] Loss: 2.7201\n",
      "[Batch 80] Loss: 3.3647\n",
      "[Batch 90] Loss: 3.4836\n",
      "[Batch 100] Loss: 3.0397\n",
      "[Batch 110] Loss: 1.6941\n",
      "[Batch 120] Loss: 2.5244\n",
      "[Batch 130] Loss: 0.1713\n",
      "[Batch 140] Loss: 1.3926\n",
      "[Batch 150] Loss: 2.9883\n",
      "[Batch 160] Loss: 1.7651\n",
      "[Batch 170] Loss: 2.4386\n",
      "[Batch 180] Loss: 0.3957\n",
      "[Batch 190] Loss: 2.2789\n",
      "[Batch 200] Loss: 4.7290\n",
      "[Batch 210] Loss: 2.5613\n",
      "[Batch 220] Loss: 2.0764\n",
      "[Batch 230] Loss: 3.0061\n",
      "[Batch 240] Loss: 2.9113\n",
      "[Batch 250] Loss: 3.2698\n",
      "[Batch 260] Loss: 2.0169\n",
      "[Batch 270] Loss: 3.9163\n",
      "[Batch 280] Loss: 3.7833\n",
      "[Batch 290] Loss: 2.0766\n",
      "[Batch 300] Loss: 3.0741\n",
      "[Batch 310] Loss: 4.3902\n",
      "[Batch 320] Loss: 2.6607\n",
      "[Batch 330] Loss: 3.1450\n",
      "[Batch 340] Loss: 3.1147\n",
      "[Batch 350] Loss: 2.6522\n",
      "[Batch 360] Loss: 1.6759\n",
      "[Batch 370] Loss: 3.8848\n",
      "[Batch 380] Loss: 3.2155\n",
      "[Batch 390] Loss: 0.9004\n",
      "[Batch 400] Loss: 3.0076\n",
      "[Batch 410] Loss: 2.3083\n",
      "[Batch 420] Loss: 3.0905\n",
      "[Batch 430] Loss: 3.4455\n",
      "[Batch 440] Loss: 0.3249\n",
      "[Batch 450] Loss: 4.4108\n",
      "[Batch 460] Loss: 3.2493\n",
      "[Batch 470] Loss: 3.2348\n",
      "[Batch 480] Loss: 0.4314\n",
      "[Batch 490] Loss: 2.6873\n",
      "[Batch 500] Loss: 2.4447\n",
      "[Batch 510] Loss: 3.2867\n",
      "[Batch 520] Loss: 3.7901\n",
      "[Batch 530] Loss: 1.1310\n",
      "[Batch 540] Loss: 3.9694\n",
      "[Batch 550] Loss: 3.6857\n",
      "[Batch 560] Loss: 2.2156\n",
      "[Batch 570] Loss: 2.8807\n",
      "[Batch 580] Loss: 1.6658\n",
      "[Batch 590] Loss: 1.6055\n",
      "[Batch 600] Loss: 1.6683\n",
      "[Batch 610] Loss: 2.0169\n",
      "[Batch 620] Loss: 1.7702\n",
      "[Batch 630] Loss: 1.8981\n",
      "[Batch 640] Loss: 2.1628\n",
      "[Batch 650] Loss: 2.5526\n",
      "[Batch 660] Loss: 3.7593\n",
      "[Batch 670] Loss: 1.8676\n",
      "[Batch 680] Loss: 1.7401\n",
      "[Batch 690] Loss: 3.4793\n",
      "[Batch 700] Loss: 2.3489\n",
      "[Batch 710] Loss: 1.7016\n",
      "[Batch 720] Loss: 3.2946\n",
      "[Batch 730] Loss: 2.2973\n",
      "[Batch 740] Loss: 2.6134\n",
      "[Batch 750] Loss: 2.7679\n",
      "[Batch 760] Loss: 1.8067\n",
      "[Batch 770] Loss: 2.7956\n",
      "[Batch 780] Loss: 4.4331\n",
      "[Batch 790] Loss: 0.9639\n",
      "[Batch 800] Loss: 4.0951\n",
      "[Batch 810] Loss: 2.7852\n",
      "[Batch 820] Loss: 2.5463\n",
      "[Batch 830] Loss: 1.9587\n",
      "[Batch 840] Loss: 3.3609\n",
      "[Batch 850] Loss: 2.2993\n",
      "[Batch 860] Loss: 2.8112\n",
      "[Batch 870] Loss: 2.1072\n",
      "[Batch 880] Loss: 1.7123\n",
      "[Batch 890] Loss: 2.2714\n",
      "[Batch 900] Loss: 1.8304\n",
      "[Batch 910] Loss: 2.4718\n",
      "[Batch 920] Loss: 1.8233\n",
      "[Batch 930] Loss: 4.1493\n",
      "[Batch 940] Loss: 4.8646\n",
      "[Batch 950] Loss: 3.7805\n",
      "[Batch 960] Loss: 2.5510\n",
      "[Batch 970] Loss: 1.1532\n",
      "[Batch 980] Loss: 3.7009\n",
      "[Batch 990] Loss: 2.8485\n",
      "[Batch 1000] Loss: 2.9541\n",
      "[Batch 1010] Loss: 2.3182\n",
      "[Batch 1020] Loss: 0.6789\n",
      "[Batch 1030] Loss: 5.0758\n",
      "[Batch 1040] Loss: 2.9254\n",
      "[Batch 1050] Loss: 3.5676\n",
      "[Batch 1060] Loss: 4.0669\n",
      "[Batch 1070] Loss: 2.7872\n",
      "[Batch 1080] Loss: 2.8821\n",
      "[Batch 1090] Loss: 1.0631\n",
      "[Batch 1100] Loss: 1.8410\n",
      "[Batch 1110] Loss: 1.1378\n",
      "[Batch 1120] Loss: 1.7554\n",
      "[Batch 1130] Loss: 1.7617\n",
      "[Batch 1140] Loss: 3.8455\n",
      "[Batch 1150] Loss: 5.2842\n",
      "[Batch 1160] Loss: 2.9772\n",
      "[Batch 1170] Loss: 5.3209\n",
      "[Batch 1180] Loss: 0.6472\n",
      "[Batch 1190] Loss: 0.7601\n",
      "[Batch 1200] Loss: 3.8406\n",
      "[Batch 1210] Loss: 1.7352\n",
      "[Batch 1220] Loss: 2.2154\n",
      "[Batch 1230] Loss: 2.9993\n",
      "[Batch 1240] Loss: 1.9507\n",
      "[Batch 1250] Loss: 1.9356\n",
      "[Batch 1260] Loss: 2.3805\n",
      "[Batch 1270] Loss: 4.3573\n",
      "[Batch 1280] Loss: 2.8107\n",
      "[Batch 1290] Loss: 0.9842\n",
      "[Batch 1300] Loss: 2.3336\n",
      "[Batch 1310] Loss: 1.6275\n",
      "[Batch 1320] Loss: 4.9063\n",
      "[Batch 1330] Loss: 2.6960\n",
      "[Batch 1340] Loss: 0.6020\n",
      "[Batch 1350] Loss: 0.4370\n",
      "[Batch 1360] Loss: 2.2870\n",
      "[Batch 1370] Loss: 1.9113\n",
      "[Batch 1380] Loss: 2.7263\n",
      "[Batch 1390] Loss: 0.6708\n",
      "[Batch 1400] Loss: 4.6113\n",
      "[Batch 1410] Loss: 3.0362\n",
      "[Batch 1420] Loss: 1.5357\n",
      "[Batch 1430] Loss: 2.9886\n",
      "[Batch 1440] Loss: 3.6629\n",
      "[Batch 1450] Loss: 2.1451\n",
      "[Batch 1460] Loss: 4.4820\n",
      "[Batch 1470] Loss: 1.9325\n",
      "[Batch 1480] Loss: 1.0205\n",
      "[Batch 1490] Loss: 3.2911\n",
      "[Batch 1500] Loss: 3.2018\n",
      "[Batch 1510] Loss: 2.1123\n",
      "[Batch 1520] Loss: 3.0659\n",
      "[Batch 1530] Loss: 1.5428\n",
      "[Batch 1540] Loss: 3.1272\n",
      "[Batch 1550] Loss: 0.8545\n",
      "[Batch 1560] Loss: 1.9148\n",
      "[Batch 1570] Loss: 1.0999\n",
      "[Batch 1580] Loss: 1.0549\n",
      "[Batch 1590] Loss: 3.4727\n",
      "[Batch 1600] Loss: 2.4340\n",
      "[Batch 1610] Loss: 1.4435\n",
      "[Batch 1620] Loss: 4.1737\n",
      "[Batch 1630] Loss: 0.7086\n",
      "[Batch 1640] Loss: 3.6819\n",
      "[Batch 1650] Loss: 2.7769\n",
      "[Batch 1660] Loss: 1.6148\n",
      "[Batch 1670] Loss: 1.6688\n",
      "[Batch 1680] Loss: 2.2494\n",
      "[Batch 1690] Loss: 2.8238\n",
      "[Batch 1700] Loss: 3.3659\n",
      "[Batch 1710] Loss: 2.9029\n",
      "[Batch 1720] Loss: 3.7045\n",
      "[Batch 1730] Loss: 3.6879\n",
      "[Batch 1740] Loss: 3.5465\n",
      "[Batch 1750] Loss: 1.2099\n",
      "[Batch 1760] Loss: 3.8808\n",
      "[Batch 1770] Loss: 2.1987\n",
      "[Batch 1780] Loss: 3.5320\n",
      "[Batch 1790] Loss: 1.4641\n",
      "[Batch 1800] Loss: 3.5426\n",
      "[Batch 1810] Loss: 3.0676\n",
      "[Batch 1820] Loss: 4.0412\n",
      "[Batch 1830] Loss: 0.9758\n",
      "[Batch 1840] Loss: 2.8780\n",
      "[Batch 1850] Loss: 1.8858\n",
      "[Batch 1860] Loss: 2.0187\n",
      "[Batch 1870] Loss: 1.9610\n",
      "[Batch 1880] Loss: 2.1259\n",
      "[Batch 1890] Loss: 2.2835\n",
      "[Batch 1900] Loss: 2.7047\n",
      "[Batch 1910] Loss: 3.4660\n",
      "[Batch 1920] Loss: 2.7065\n",
      "[Batch 1930] Loss: 2.2371\n",
      "[Batch 1940] Loss: 2.0141\n",
      "[Batch 1950] Loss: 2.2426\n",
      "[Batch 1960] Loss: 1.5455\n",
      "[Batch 1970] Loss: 2.0615\n",
      "[Batch 1980] Loss: 3.9549\n",
      "[Batch 1990] Loss: 3.5026\n",
      "[Batch 2000] Loss: 1.6187\n",
      "[Batch 2010] Loss: 2.2881\n",
      "[Batch 2020] Loss: 2.5821\n",
      "[Batch 2030] Loss: 3.2306\n",
      "[Batch 2040] Loss: 2.5886\n",
      "[Batch 2050] Loss: 2.4323\n",
      "[Batch 2060] Loss: 2.5406\n",
      "[Batch 2070] Loss: 2.8351\n",
      "[Batch 2080] Loss: 3.2064\n",
      "[Batch 2090] Loss: 2.9845\n",
      "[Batch 2100] Loss: 0.7995\n",
      "[Batch 2110] Loss: 2.2970\n",
      "[Batch 2120] Loss: 3.7399\n",
      "[Batch 2130] Loss: 3.5556\n",
      "[Batch 2140] Loss: 2.6045\n",
      "[Batch 2150] Loss: 2.3339\n",
      "[Batch 2160] Loss: 0.8088\n",
      "[Batch 2170] Loss: 3.9423\n",
      "[Batch 2180] Loss: 1.9776\n",
      "[Batch 2190] Loss: 3.1193\n",
      "[Batch 2200] Loss: 3.5939\n",
      "[Batch 2210] Loss: 1.6709\n",
      "[Batch 2220] Loss: 0.5962\n",
      "[Batch 2230] Loss: 2.3597\n",
      "[Batch 2240] Loss: 1.8169\n",
      "Epoch 6/10 | Loss: 5874.7431\n",
      "[Batch 0] Loss: 1.7522\n",
      "[Batch 10] Loss: 1.6520\n",
      "[Batch 20] Loss: 2.2220\n",
      "[Batch 30] Loss: 1.5380\n",
      "[Batch 40] Loss: 2.6790\n",
      "[Batch 50] Loss: 0.4748\n",
      "[Batch 60] Loss: 4.2672\n",
      "[Batch 70] Loss: 2.6998\n",
      "[Batch 80] Loss: 2.0709\n",
      "[Batch 90] Loss: 2.6091\n",
      "[Batch 100] Loss: 3.5188\n",
      "[Batch 110] Loss: 1.6846\n",
      "[Batch 120] Loss: 1.7577\n",
      "[Batch 130] Loss: 1.5035\n",
      "[Batch 140] Loss: 2.9586\n",
      "[Batch 150] Loss: 2.3484\n",
      "[Batch 160] Loss: 2.5549\n",
      "[Batch 170] Loss: 3.0212\n",
      "[Batch 180] Loss: 1.9870\n",
      "[Batch 190] Loss: 1.8345\n",
      "[Batch 200] Loss: 1.9207\n",
      "[Batch 210] Loss: 3.2571\n",
      "[Batch 220] Loss: 3.5830\n",
      "[Batch 230] Loss: 1.1913\n",
      "[Batch 240] Loss: 3.0315\n",
      "[Batch 250] Loss: 2.6639\n",
      "[Batch 260] Loss: 1.4638\n",
      "[Batch 270] Loss: 2.1570\n",
      "[Batch 280] Loss: 0.7772\n",
      "[Batch 290] Loss: 3.2328\n",
      "[Batch 300] Loss: 1.2524\n",
      "[Batch 310] Loss: 4.8868\n",
      "[Batch 320] Loss: 3.0449\n",
      "[Batch 330] Loss: 2.1577\n",
      "[Batch 340] Loss: 1.8311\n",
      "[Batch 350] Loss: 1.8712\n",
      "[Batch 360] Loss: 4.0645\n",
      "[Batch 370] Loss: 1.5505\n",
      "[Batch 380] Loss: 1.7588\n",
      "[Batch 390] Loss: 1.9449\n",
      "[Batch 400] Loss: 1.1750\n",
      "[Batch 410] Loss: 4.1674\n",
      "[Batch 420] Loss: 2.6152\n",
      "[Batch 430] Loss: 0.3514\n",
      "[Batch 440] Loss: 3.4138\n",
      "[Batch 450] Loss: 1.8806\n",
      "[Batch 460] Loss: 2.6539\n",
      "[Batch 470] Loss: 3.9520\n",
      "[Batch 480] Loss: 2.0675\n",
      "[Batch 490] Loss: 1.8740\n",
      "[Batch 500] Loss: 2.2320\n",
      "[Batch 510] Loss: 0.5999\n",
      "[Batch 520] Loss: 2.9816\n",
      "[Batch 530] Loss: 1.7232\n",
      "[Batch 540] Loss: 3.1333\n",
      "[Batch 550] Loss: 2.4878\n",
      "[Batch 560] Loss: 0.8797\n",
      "[Batch 570] Loss: 5.0680\n",
      "[Batch 580] Loss: 2.7331\n",
      "[Batch 590] Loss: 1.6507\n",
      "[Batch 600] Loss: 0.9991\n",
      "[Batch 610] Loss: 3.2049\n",
      "[Batch 620] Loss: 1.4702\n",
      "[Batch 630] Loss: 3.2711\n",
      "[Batch 640] Loss: 5.0777\n",
      "[Batch 650] Loss: 2.9064\n",
      "[Batch 660] Loss: 2.1153\n",
      "[Batch 670] Loss: 1.4848\n",
      "[Batch 680] Loss: 3.4858\n",
      "[Batch 690] Loss: 2.5921\n",
      "[Batch 700] Loss: 3.3222\n",
      "[Batch 710] Loss: 3.4238\n",
      "[Batch 720] Loss: 1.5917\n",
      "[Batch 730] Loss: 1.7495\n",
      "[Batch 740] Loss: 2.5236\n",
      "[Batch 750] Loss: 1.8182\n",
      "[Batch 760] Loss: 2.2216\n",
      "[Batch 770] Loss: 1.2264\n",
      "[Batch 780] Loss: 4.9620\n",
      "[Batch 790] Loss: 3.7779\n",
      "[Batch 800] Loss: 3.0308\n",
      "[Batch 810] Loss: 0.2962\n",
      "[Batch 820] Loss: 3.1611\n",
      "[Batch 830] Loss: 2.4701\n",
      "[Batch 840] Loss: 2.6106\n",
      "[Batch 850] Loss: 2.2811\n",
      "[Batch 860] Loss: 1.3369\n",
      "[Batch 870] Loss: 3.0097\n",
      "[Batch 880] Loss: 1.4619\n",
      "[Batch 890] Loss: 3.5974\n",
      "[Batch 900] Loss: 3.3601\n",
      "[Batch 910] Loss: 2.6606\n",
      "[Batch 920] Loss: 1.4441\n",
      "[Batch 930] Loss: 1.8790\n",
      "[Batch 940] Loss: 1.8048\n",
      "[Batch 950] Loss: 1.4084\n",
      "[Batch 960] Loss: 1.3416\n",
      "[Batch 970] Loss: 4.0137\n",
      "[Batch 980] Loss: 1.2084\n",
      "[Batch 990] Loss: 3.7083\n",
      "[Batch 1000] Loss: 2.0766\n",
      "[Batch 1010] Loss: 2.0961\n",
      "[Batch 1020] Loss: 2.2899\n",
      "[Batch 1030] Loss: 2.1951\n",
      "[Batch 1040] Loss: 3.5439\n",
      "[Batch 1050] Loss: 2.4368\n",
      "[Batch 1060] Loss: 1.7161\n",
      "[Batch 1070] Loss: 2.6187\n",
      "[Batch 1080] Loss: 2.3803\n",
      "[Batch 1090] Loss: 2.5933\n",
      "[Batch 1100] Loss: 2.3355\n",
      "[Batch 1110] Loss: 1.9802\n",
      "[Batch 1120] Loss: 2.0138\n",
      "[Batch 1130] Loss: 1.9463\n",
      "[Batch 1140] Loss: 0.6610\n",
      "[Batch 1150] Loss: 1.6284\n",
      "[Batch 1160] Loss: 2.0515\n",
      "[Batch 1170] Loss: 2.4439\n",
      "[Batch 1180] Loss: 1.9509\n",
      "[Batch 1190] Loss: 2.4777\n",
      "[Batch 1200] Loss: 2.2896\n",
      "[Batch 1210] Loss: 2.9198\n",
      "[Batch 1220] Loss: 2.3231\n",
      "[Batch 1230] Loss: 1.1427\n",
      "[Batch 1240] Loss: 2.7193\n",
      "[Batch 1250] Loss: 3.2258\n",
      "[Batch 1260] Loss: 2.5320\n",
      "[Batch 1270] Loss: 4.9358\n",
      "[Batch 1280] Loss: 1.8640\n",
      "[Batch 1290] Loss: 1.7799\n",
      "[Batch 1300] Loss: 3.1131\n",
      "[Batch 1310] Loss: 2.0604\n",
      "[Batch 1320] Loss: 3.0251\n",
      "[Batch 1330] Loss: 2.6122\n",
      "[Batch 1340] Loss: 0.9280\n",
      "[Batch 1350] Loss: 0.3052\n",
      "[Batch 1360] Loss: 1.8422\n",
      "[Batch 1370] Loss: 3.7019\n",
      "[Batch 1380] Loss: 0.5073\n",
      "[Batch 1390] Loss: 3.4258\n",
      "[Batch 1400] Loss: 2.6467\n",
      "[Batch 1410] Loss: 1.7042\n",
      "[Batch 1420] Loss: 1.2170\n",
      "[Batch 1430] Loss: 3.7834\n",
      "[Batch 1440] Loss: 4.7337\n",
      "[Batch 1450] Loss: 0.8203\n",
      "[Batch 1460] Loss: 4.1011\n",
      "[Batch 1470] Loss: 1.1306\n",
      "[Batch 1480] Loss: 1.7128\n",
      "[Batch 1490] Loss: 1.7676\n",
      "[Batch 1500] Loss: 0.8152\n",
      "[Batch 1510] Loss: 0.6657\n",
      "[Batch 1520] Loss: 2.4985\n",
      "[Batch 1530] Loss: 1.8242\n",
      "[Batch 1540] Loss: 4.3345\n",
      "[Batch 1550] Loss: 3.4425\n",
      "[Batch 1560] Loss: 4.5446\n",
      "[Batch 1570] Loss: 2.5377\n",
      "[Batch 1580] Loss: 3.0611\n",
      "[Batch 1590] Loss: 5.0497\n",
      "[Batch 1600] Loss: 1.3344\n",
      "[Batch 1610] Loss: 3.0869\n",
      "[Batch 1620] Loss: 1.3434\n",
      "[Batch 1630] Loss: 0.5595\n",
      "[Batch 1640] Loss: 1.2977\n",
      "[Batch 1650] Loss: 1.2285\n",
      "[Batch 1660] Loss: 2.3047\n",
      "[Batch 1670] Loss: 2.3245\n",
      "[Batch 1680] Loss: 3.1123\n",
      "[Batch 1690] Loss: 3.6487\n",
      "[Batch 1700] Loss: 1.9955\n",
      "[Batch 1710] Loss: 3.8865\n",
      "[Batch 1720] Loss: 2.5124\n",
      "[Batch 1730] Loss: 4.9264\n",
      "[Batch 1740] Loss: 2.9364\n",
      "[Batch 1750] Loss: 2.6346\n",
      "[Batch 1760] Loss: 1.1683\n",
      "[Batch 1770] Loss: 3.0319\n",
      "[Batch 1780] Loss: 1.9558\n",
      "[Batch 1790] Loss: 3.4939\n",
      "[Batch 1800] Loss: 4.1737\n",
      "[Batch 1810] Loss: 1.5733\n",
      "[Batch 1820] Loss: 1.7448\n",
      "[Batch 1830] Loss: 1.6135\n",
      "[Batch 1840] Loss: 3.2743\n",
      "[Batch 1850] Loss: 2.5803\n",
      "[Batch 1860] Loss: 1.9480\n",
      "[Batch 1870] Loss: 0.2160\n",
      "[Batch 1880] Loss: 3.1059\n",
      "[Batch 1890] Loss: 3.1884\n",
      "[Batch 1900] Loss: 1.1789\n",
      "[Batch 1910] Loss: 2.5779\n",
      "[Batch 1920] Loss: 3.3476\n",
      "[Batch 1930] Loss: 3.5601\n",
      "[Batch 1940] Loss: 2.2428\n",
      "[Batch 1950] Loss: 1.9502\n",
      "[Batch 1960] Loss: 1.5521\n",
      "[Batch 1970] Loss: 1.0873\n",
      "[Batch 1980] Loss: 1.5299\n",
      "[Batch 1990] Loss: 2.5653\n",
      "[Batch 2000] Loss: 1.0595\n",
      "[Batch 2010] Loss: 1.8829\n",
      "[Batch 2020] Loss: 0.4348\n",
      "[Batch 2030] Loss: 1.3215\n",
      "[Batch 2040] Loss: 3.3499\n",
      "[Batch 2050] Loss: 1.3909\n",
      "[Batch 2060] Loss: 3.4263\n",
      "[Batch 2070] Loss: 2.2767\n",
      "[Batch 2080] Loss: 3.2535\n",
      "[Batch 2090] Loss: 0.6174\n",
      "[Batch 2100] Loss: 0.7041\n",
      "[Batch 2110] Loss: 2.2400\n",
      "[Batch 2120] Loss: 1.4306\n",
      "[Batch 2130] Loss: 1.4509\n",
      "[Batch 2140] Loss: 3.2040\n",
      "[Batch 2150] Loss: 5.8968\n",
      "[Batch 2160] Loss: 0.3381\n",
      "[Batch 2170] Loss: 1.6868\n",
      "[Batch 2180] Loss: 1.8909\n",
      "[Batch 2190] Loss: 1.3521\n",
      "[Batch 2200] Loss: 0.6870\n",
      "[Batch 2210] Loss: 4.4710\n",
      "[Batch 2220] Loss: 1.7127\n",
      "[Batch 2230] Loss: 3.0803\n",
      "[Batch 2240] Loss: 1.4380\n",
      "Epoch 7/10 | Loss: 5286.7147\n",
      "[Batch 0] Loss: 1.1065\n",
      "[Batch 10] Loss: 2.8129\n",
      "[Batch 20] Loss: 1.8742\n",
      "[Batch 30] Loss: 3.6213\n",
      "[Batch 40] Loss: 3.8308\n",
      "[Batch 50] Loss: 2.1479\n",
      "[Batch 60] Loss: 1.6972\n",
      "[Batch 70] Loss: 1.0830\n",
      "[Batch 80] Loss: 2.7901\n",
      "[Batch 90] Loss: 2.6592\n",
      "[Batch 100] Loss: 2.1952\n",
      "[Batch 110] Loss: 1.6882\n",
      "[Batch 120] Loss: 1.7869\n",
      "[Batch 130] Loss: 3.2677\n",
      "[Batch 140] Loss: 2.7535\n",
      "[Batch 150] Loss: 0.5761\n",
      "[Batch 160] Loss: 2.3269\n",
      "[Batch 170] Loss: 3.7779\n",
      "[Batch 180] Loss: 3.4092\n",
      "[Batch 190] Loss: 1.8900\n",
      "[Batch 200] Loss: 1.9011\n",
      "[Batch 210] Loss: 0.9858\n",
      "[Batch 220] Loss: 2.5533\n",
      "[Batch 230] Loss: 0.8583\n",
      "[Batch 240] Loss: 3.6545\n",
      "[Batch 250] Loss: 3.5485\n",
      "[Batch 260] Loss: 1.8732\n",
      "[Batch 270] Loss: 0.9370\n",
      "[Batch 280] Loss: 3.0322\n",
      "[Batch 290] Loss: 1.7486\n",
      "[Batch 300] Loss: 1.4657\n",
      "[Batch 310] Loss: 3.1849\n",
      "[Batch 320] Loss: 2.6417\n",
      "[Batch 330] Loss: 1.3710\n",
      "[Batch 340] Loss: 2.0418\n",
      "[Batch 350] Loss: 3.6175\n",
      "[Batch 360] Loss: 1.4146\n",
      "[Batch 370] Loss: 2.6267\n",
      "[Batch 380] Loss: 3.4113\n",
      "[Batch 390] Loss: 3.1535\n",
      "[Batch 400] Loss: 1.1719\n",
      "[Batch 410] Loss: 2.9978\n",
      "[Batch 420] Loss: 1.1037\n",
      "[Batch 430] Loss: 1.3577\n",
      "[Batch 440] Loss: 0.7901\n",
      "[Batch 450] Loss: 1.7136\n",
      "[Batch 460] Loss: 0.4189\n",
      "[Batch 470] Loss: 2.6186\n",
      "[Batch 480] Loss: 3.3238\n",
      "[Batch 490] Loss: 1.0654\n",
      "[Batch 500] Loss: 2.8050\n",
      "[Batch 510] Loss: 2.9240\n",
      "[Batch 520] Loss: 1.5982\n",
      "[Batch 530] Loss: 3.3449\n",
      "[Batch 540] Loss: 2.0400\n",
      "[Batch 550] Loss: 1.5750\n",
      "[Batch 560] Loss: 2.3536\n",
      "[Batch 570] Loss: 3.1178\n",
      "[Batch 580] Loss: 1.5948\n",
      "[Batch 590] Loss: 3.3565\n",
      "[Batch 600] Loss: 4.3121\n",
      "[Batch 610] Loss: 1.8001\n",
      "[Batch 620] Loss: 0.8572\n",
      "[Batch 630] Loss: 3.1823\n",
      "[Batch 640] Loss: 1.6675\n",
      "[Batch 650] Loss: 1.8019\n",
      "[Batch 660] Loss: 0.8869\n",
      "[Batch 670] Loss: 0.4174\n",
      "[Batch 680] Loss: 2.2236\n",
      "[Batch 690] Loss: 0.7481\n",
      "[Batch 700] Loss: 1.9868\n",
      "[Batch 710] Loss: 2.3559\n",
      "[Batch 720] Loss: 4.2061\n",
      "[Batch 730] Loss: 0.7175\n",
      "[Batch 740] Loss: 1.2933\n",
      "[Batch 750] Loss: 2.4966\n",
      "[Batch 760] Loss: 1.1124\n",
      "[Batch 770] Loss: 0.8139\n",
      "[Batch 780] Loss: 1.6272\n",
      "[Batch 790] Loss: 1.9540\n",
      "[Batch 800] Loss: 1.7386\n",
      "[Batch 810] Loss: 1.1191\n",
      "[Batch 820] Loss: 2.3825\n",
      "[Batch 830] Loss: 0.8085\n",
      "[Batch 840] Loss: 2.3184\n",
      "[Batch 850] Loss: 1.8910\n",
      "[Batch 860] Loss: 2.3995\n",
      "[Batch 870] Loss: 4.7531\n",
      "[Batch 880] Loss: 2.9277\n",
      "[Batch 890] Loss: 4.0886\n",
      "[Batch 900] Loss: 1.7916\n",
      "[Batch 910] Loss: 2.8228\n",
      "[Batch 920] Loss: 2.9715\n",
      "[Batch 930] Loss: 2.2538\n",
      "[Batch 940] Loss: 2.2082\n",
      "[Batch 950] Loss: 0.6887\n",
      "[Batch 960] Loss: 2.5665\n",
      "[Batch 970] Loss: 1.6142\n",
      "[Batch 980] Loss: 2.9857\n",
      "[Batch 990] Loss: 4.5166\n",
      "[Batch 1000] Loss: 2.9475\n",
      "[Batch 1010] Loss: 1.8623\n",
      "[Batch 1020] Loss: 2.0248\n",
      "[Batch 1030] Loss: 1.4649\n",
      "[Batch 1040] Loss: 4.4038\n",
      "[Batch 1050] Loss: 2.7469\n",
      "[Batch 1060] Loss: 1.8999\n",
      "[Batch 1070] Loss: 3.4003\n",
      "[Batch 1080] Loss: 3.0932\n",
      "[Batch 1090] Loss: 3.6150\n",
      "[Batch 1100] Loss: 1.8232\n",
      "[Batch 1110] Loss: 1.3598\n",
      "[Batch 1120] Loss: 2.3064\n",
      "[Batch 1130] Loss: 1.4649\n",
      "[Batch 1140] Loss: 4.1491\n",
      "[Batch 1150] Loss: 2.7068\n",
      "[Batch 1160] Loss: 1.1162\n",
      "[Batch 1170] Loss: 1.6650\n",
      "[Batch 1180] Loss: 1.9034\n",
      "[Batch 1190] Loss: 0.9622\n",
      "[Batch 1200] Loss: 1.1382\n",
      "[Batch 1210] Loss: 2.0401\n",
      "[Batch 1220] Loss: 1.8742\n",
      "[Batch 1230] Loss: 3.4518\n",
      "[Batch 1240] Loss: 1.8529\n",
      "[Batch 1250] Loss: 2.9244\n",
      "[Batch 1260] Loss: 0.4890\n",
      "[Batch 1270] Loss: 1.6846\n",
      "[Batch 1280] Loss: 1.9031\n",
      "[Batch 1290] Loss: 3.7014\n",
      "[Batch 1300] Loss: 0.4045\n",
      "[Batch 1310] Loss: 2.0967\n",
      "[Batch 1320] Loss: 1.6750\n",
      "[Batch 1330] Loss: 2.1577\n",
      "[Batch 1340] Loss: 2.9084\n",
      "[Batch 1350] Loss: 1.3771\n",
      "[Batch 1360] Loss: 1.7802\n",
      "[Batch 1370] Loss: 0.4339\n",
      "[Batch 1380] Loss: 0.6446\n",
      "[Batch 1390] Loss: 2.1217\n",
      "[Batch 1400] Loss: 1.1590\n",
      "[Batch 1410] Loss: 0.6569\n",
      "[Batch 1420] Loss: 2.8169\n",
      "[Batch 1430] Loss: 1.4664\n",
      "[Batch 1440] Loss: 3.2078\n",
      "[Batch 1450] Loss: 2.6861\n",
      "[Batch 1460] Loss: 0.2787\n",
      "[Batch 1470] Loss: 1.6629\n",
      "[Batch 1480] Loss: 2.1237\n",
      "[Batch 1490] Loss: 1.4231\n",
      "[Batch 1500] Loss: 3.8887\n",
      "[Batch 1510] Loss: 1.9917\n",
      "[Batch 1520] Loss: 3.6576\n",
      "[Batch 1530] Loss: 2.5262\n",
      "[Batch 1540] Loss: 3.2995\n",
      "[Batch 1550] Loss: 4.3326\n",
      "[Batch 1560] Loss: 3.4348\n",
      "[Batch 1570] Loss: 1.1428\n",
      "[Batch 1580] Loss: 1.5032\n",
      "[Batch 1590] Loss: 2.1857\n",
      "[Batch 1600] Loss: 1.5997\n",
      "[Batch 1610] Loss: 2.6902\n",
      "[Batch 1620] Loss: 2.3779\n",
      "[Batch 1630] Loss: 2.7734\n",
      "[Batch 1640] Loss: 1.2192\n",
      "[Batch 1650] Loss: 2.3792\n",
      "[Batch 1660] Loss: 1.5949\n",
      "[Batch 1670] Loss: 1.4319\n",
      "[Batch 1680] Loss: 3.4187\n",
      "[Batch 1690] Loss: 1.2334\n",
      "[Batch 1700] Loss: 2.5381\n",
      "[Batch 1710] Loss: 1.1562\n",
      "[Batch 1720] Loss: 0.5460\n",
      "[Batch 1730] Loss: 2.5906\n",
      "[Batch 1740] Loss: 1.5856\n",
      "[Batch 1750] Loss: 2.9685\n",
      "[Batch 1760] Loss: 2.5938\n",
      "[Batch 1770] Loss: 1.3840\n",
      "[Batch 1780] Loss: 0.6091\n",
      "[Batch 1790] Loss: 2.2752\n",
      "[Batch 1800] Loss: 2.8405\n",
      "[Batch 1810] Loss: 2.5348\n",
      "[Batch 1820] Loss: 2.7674\n",
      "[Batch 1830] Loss: 2.9818\n",
      "[Batch 1840] Loss: 3.4129\n",
      "[Batch 1850] Loss: 1.3876\n",
      "[Batch 1860] Loss: 2.9890\n",
      "[Batch 1870] Loss: 2.4427\n",
      "[Batch 1880] Loss: 2.4806\n",
      "[Batch 1890] Loss: 4.0810\n",
      "[Batch 1900] Loss: 2.8101\n",
      "[Batch 1910] Loss: 1.1863\n",
      "[Batch 1920] Loss: 3.6275\n",
      "[Batch 1930] Loss: 2.0513\n",
      "[Batch 1940] Loss: 0.7493\n",
      "[Batch 1950] Loss: 2.8369\n",
      "[Batch 1960] Loss: 0.9655\n",
      "[Batch 1970] Loss: 1.9702\n",
      "[Batch 1980] Loss: 0.5240\n",
      "[Batch 1990] Loss: 2.6877\n",
      "[Batch 2000] Loss: 0.8621\n",
      "[Batch 2010] Loss: 2.1492\n",
      "[Batch 2020] Loss: 2.3278\n",
      "[Batch 2030] Loss: 1.4044\n",
      "[Batch 2040] Loss: 1.3924\n",
      "[Batch 2050] Loss: 1.0087\n",
      "[Batch 2060] Loss: 2.7890\n",
      "[Batch 2070] Loss: 3.0324\n",
      "[Batch 2080] Loss: 0.5197\n",
      "[Batch 2090] Loss: 1.3881\n",
      "[Batch 2100] Loss: 1.1327\n",
      "[Batch 2110] Loss: 0.3138\n",
      "[Batch 2120] Loss: 2.3725\n",
      "[Batch 2130] Loss: 4.2996\n",
      "[Batch 2140] Loss: 1.4871\n",
      "[Batch 2150] Loss: 1.0325\n",
      "[Batch 2160] Loss: 2.0925\n",
      "[Batch 2170] Loss: 1.5102\n",
      "[Batch 2180] Loss: 0.9350\n",
      "[Batch 2190] Loss: 0.9495\n",
      "[Batch 2200] Loss: 1.3644\n",
      "[Batch 2210] Loss: 1.5819\n",
      "[Batch 2220] Loss: 3.6793\n",
      "[Batch 2230] Loss: 2.6968\n",
      "[Batch 2240] Loss: 1.4702\n",
      "Epoch 8/10 | Loss: 4746.3038\n",
      "[Batch 0] Loss: 1.8192\n",
      "[Batch 10] Loss: 2.2161\n",
      "[Batch 20] Loss: 1.5545\n",
      "[Batch 30] Loss: 0.5260\n",
      "[Batch 40] Loss: 2.8440\n",
      "[Batch 50] Loss: 0.9145\n",
      "[Batch 60] Loss: 1.2757\n",
      "[Batch 70] Loss: 0.4530\n",
      "[Batch 80] Loss: 1.3981\n",
      "[Batch 90] Loss: 1.3966\n",
      "[Batch 100] Loss: 2.0700\n",
      "[Batch 110] Loss: 1.2984\n",
      "[Batch 120] Loss: 3.0278\n",
      "[Batch 130] Loss: 2.0412\n",
      "[Batch 140] Loss: 1.2436\n",
      "[Batch 150] Loss: 0.8078\n",
      "[Batch 160] Loss: 2.1620\n",
      "[Batch 170] Loss: 0.4189\n",
      "[Batch 180] Loss: 0.6468\n",
      "[Batch 190] Loss: 2.1024\n",
      "[Batch 200] Loss: 0.6934\n",
      "[Batch 210] Loss: 2.5184\n",
      "[Batch 220] Loss: 2.1175\n",
      "[Batch 230] Loss: 0.5394\n",
      "[Batch 240] Loss: 0.3568\n",
      "[Batch 250] Loss: 1.0820\n",
      "[Batch 260] Loss: 2.4679\n",
      "[Batch 270] Loss: 1.8091\n",
      "[Batch 280] Loss: 2.0723\n",
      "[Batch 290] Loss: 1.8384\n",
      "[Batch 300] Loss: 1.1700\n",
      "[Batch 310] Loss: 1.3878\n",
      "[Batch 320] Loss: 1.6891\n",
      "[Batch 330] Loss: 1.0943\n",
      "[Batch 340] Loss: 2.5686\n",
      "[Batch 350] Loss: 1.8478\n",
      "[Batch 360] Loss: 1.3857\n",
      "[Batch 370] Loss: 1.8783\n",
      "[Batch 380] Loss: 2.9899\n",
      "[Batch 390] Loss: 0.4405\n",
      "[Batch 400] Loss: 2.0941\n",
      "[Batch 410] Loss: 1.0210\n",
      "[Batch 420] Loss: 2.7980\n",
      "[Batch 430] Loss: 1.4340\n",
      "[Batch 440] Loss: 2.7635\n",
      "[Batch 450] Loss: 1.6295\n",
      "[Batch 460] Loss: 2.1950\n",
      "[Batch 470] Loss: 2.4199\n",
      "[Batch 480] Loss: 2.1251\n",
      "[Batch 490] Loss: 2.1661\n",
      "[Batch 500] Loss: 0.6320\n",
      "[Batch 510] Loss: 2.1483\n",
      "[Batch 520] Loss: 1.6645\n",
      "[Batch 530] Loss: 2.1404\n",
      "[Batch 540] Loss: 0.4786\n",
      "[Batch 550] Loss: 2.5542\n",
      "[Batch 560] Loss: 1.1438\n",
      "[Batch 570] Loss: 1.5368\n",
      "[Batch 580] Loss: 2.2206\n",
      "[Batch 590] Loss: 0.5265\n",
      "[Batch 600] Loss: 3.7268\n",
      "[Batch 610] Loss: 1.4322\n",
      "[Batch 620] Loss: 1.7871\n",
      "[Batch 630] Loss: 1.4319\n",
      "[Batch 640] Loss: 0.5452\n",
      "[Batch 650] Loss: 1.2310\n",
      "[Batch 660] Loss: 1.3108\n",
      "[Batch 670] Loss: 2.1717\n",
      "[Batch 680] Loss: 2.2537\n",
      "[Batch 690] Loss: 0.8186\n",
      "[Batch 700] Loss: 0.6391\n",
      "[Batch 710] Loss: 1.2361\n",
      "[Batch 720] Loss: 0.5157\n",
      "[Batch 730] Loss: 1.6904\n",
      "[Batch 740] Loss: 0.9597\n",
      "[Batch 750] Loss: 0.4585\n",
      "[Batch 760] Loss: 2.2954\n",
      "[Batch 770] Loss: 1.1417\n",
      "[Batch 780] Loss: 1.5020\n",
      "[Batch 790] Loss: 2.4199\n",
      "[Batch 800] Loss: 0.2503\n",
      "[Batch 810] Loss: 0.6112\n",
      "[Batch 820] Loss: 2.2232\n",
      "[Batch 830] Loss: 2.2995\n",
      "[Batch 840] Loss: 1.8438\n",
      "[Batch 850] Loss: 1.3780\n",
      "[Batch 860] Loss: 1.1936\n",
      "[Batch 870] Loss: 2.2824\n",
      "[Batch 880] Loss: 1.8050\n",
      "[Batch 890] Loss: 0.6311\n",
      "[Batch 900] Loss: 1.6416\n",
      "[Batch 910] Loss: 1.1828\n",
      "[Batch 920] Loss: 2.4277\n",
      "[Batch 930] Loss: 2.2160\n",
      "[Batch 940] Loss: 2.1742\n",
      "[Batch 950] Loss: 1.0290\n",
      "[Batch 960] Loss: 2.0470\n",
      "[Batch 970] Loss: 1.9356\n",
      "[Batch 980] Loss: 2.6547\n",
      "[Batch 990] Loss: 1.2789\n",
      "[Batch 1000] Loss: 0.5354\n",
      "[Batch 1010] Loss: 2.0317\n",
      "[Batch 1020] Loss: 1.1480\n",
      "[Batch 1030] Loss: 2.8781\n",
      "[Batch 1040] Loss: 0.3124\n",
      "[Batch 1050] Loss: 2.7848\n",
      "[Batch 1060] Loss: 2.3745\n",
      "[Batch 1070] Loss: 2.4524\n",
      "[Batch 1080] Loss: 3.1041\n",
      "[Batch 1090] Loss: 1.6315\n",
      "[Batch 1100] Loss: 2.1391\n",
      "[Batch 1110] Loss: 3.1747\n",
      "[Batch 1120] Loss: 0.2614\n",
      "[Batch 1130] Loss: 1.3074\n",
      "[Batch 1140] Loss: 0.6473\n",
      "[Batch 1150] Loss: 2.4339\n",
      "[Batch 1160] Loss: 3.4329\n",
      "[Batch 1170] Loss: 1.3900\n",
      "[Batch 1180] Loss: 1.4996\n",
      "[Batch 1190] Loss: 2.1891\n",
      "[Batch 1200] Loss: 1.5290\n",
      "[Batch 1210] Loss: 1.5534\n",
      "[Batch 1220] Loss: 1.7320\n",
      "[Batch 1230] Loss: 0.9203\n",
      "[Batch 1240] Loss: 1.5863\n",
      "[Batch 1250] Loss: 2.8753\n",
      "[Batch 1260] Loss: 0.6281\n",
      "[Batch 1270] Loss: 2.9597\n",
      "[Batch 1280] Loss: 0.2591\n",
      "[Batch 1290] Loss: 2.5976\n",
      "[Batch 1300] Loss: 1.7041\n",
      "[Batch 1310] Loss: 2.8082\n",
      "[Batch 1320] Loss: 1.1972\n",
      "[Batch 1330] Loss: 3.0961\n",
      "[Batch 1340] Loss: 2.0556\n",
      "[Batch 1350] Loss: 1.8984\n",
      "[Batch 1360] Loss: 1.0709\n",
      "[Batch 1370] Loss: 2.0712\n",
      "[Batch 1380] Loss: 0.3102\n",
      "[Batch 1390] Loss: 1.3246\n",
      "[Batch 1400] Loss: 1.9423\n",
      "[Batch 1410] Loss: 2.2327\n",
      "[Batch 1420] Loss: 2.0630\n",
      "[Batch 1430] Loss: 1.5339\n",
      "[Batch 1440] Loss: 2.3980\n",
      "[Batch 1450] Loss: 1.8277\n",
      "[Batch 1460] Loss: 2.5415\n",
      "[Batch 1470] Loss: 2.2053\n",
      "[Batch 1480] Loss: 1.7868\n",
      "[Batch 1490] Loss: 2.1799\n",
      "[Batch 1500] Loss: 2.1403\n",
      "[Batch 1510] Loss: 4.2442\n",
      "[Batch 1520] Loss: 3.6614\n",
      "[Batch 1530] Loss: 2.5112\n",
      "[Batch 1540] Loss: 1.9401\n",
      "[Batch 1550] Loss: 2.2736\n",
      "[Batch 1560] Loss: 0.3983\n",
      "[Batch 1570] Loss: 2.3675\n",
      "[Batch 1580] Loss: 1.6708\n",
      "[Batch 1590] Loss: 2.9778\n",
      "[Batch 1600] Loss: 2.2148\n",
      "[Batch 1610] Loss: 4.6688\n",
      "[Batch 1620] Loss: 2.6064\n",
      "[Batch 1630] Loss: 1.8592\n",
      "[Batch 1640] Loss: 0.3110\n",
      "[Batch 1650] Loss: 0.9032\n",
      "[Batch 1660] Loss: 2.1134\n",
      "[Batch 1670] Loss: 3.2807\n",
      "[Batch 1680] Loss: 1.2431\n",
      "[Batch 1690] Loss: 1.6006\n",
      "[Batch 1700] Loss: 0.8358\n",
      "[Batch 1710] Loss: 1.3193\n",
      "[Batch 1720] Loss: 2.1265\n",
      "[Batch 1730] Loss: 2.2555\n",
      "[Batch 1740] Loss: 0.8234\n",
      "[Batch 1750] Loss: 0.5019\n",
      "[Batch 1760] Loss: 1.1477\n",
      "[Batch 1770] Loss: 1.8814\n",
      "[Batch 1780] Loss: 1.0977\n",
      "[Batch 1790] Loss: 0.4990\n",
      "[Batch 1800] Loss: 2.4565\n",
      "[Batch 1810] Loss: 3.3682\n",
      "[Batch 1820] Loss: 0.6319\n",
      "[Batch 1830] Loss: 0.7667\n",
      "[Batch 1840] Loss: 3.7687\n",
      "[Batch 1850] Loss: 0.5110\n",
      "[Batch 1860] Loss: 2.2419\n",
      "[Batch 1870] Loss: 1.3028\n",
      "[Batch 1880] Loss: 2.4254\n",
      "[Batch 1890] Loss: 1.4996\n",
      "[Batch 1900] Loss: 2.8852\n",
      "[Batch 1910] Loss: 1.2797\n",
      "[Batch 1920] Loss: 2.9553\n",
      "[Batch 1930] Loss: 2.3258\n",
      "[Batch 1940] Loss: 1.0976\n",
      "[Batch 1950] Loss: 1.4596\n",
      "[Batch 1960] Loss: 1.6346\n",
      "[Batch 1970] Loss: 2.7402\n",
      "[Batch 1980] Loss: 0.2988\n",
      "[Batch 1990] Loss: 2.4963\n",
      "[Batch 2000] Loss: 0.6898\n",
      "[Batch 2010] Loss: 2.1887\n",
      "[Batch 2020] Loss: 2.9117\n",
      "[Batch 2030] Loss: 3.3320\n",
      "[Batch 2040] Loss: 0.5852\n",
      "[Batch 2050] Loss: 4.0109\n",
      "[Batch 2060] Loss: 2.8591\n",
      "[Batch 2070] Loss: 2.6324\n",
      "[Batch 2080] Loss: 0.9673\n",
      "[Batch 2090] Loss: 0.4974\n",
      "[Batch 2100] Loss: 0.7371\n",
      "[Batch 2110] Loss: 1.1535\n",
      "[Batch 2120] Loss: 0.9350\n",
      "[Batch 2130] Loss: 2.4935\n",
      "[Batch 2140] Loss: 1.2063\n",
      "[Batch 2150] Loss: 4.3646\n",
      "[Batch 2160] Loss: 2.3897\n",
      "[Batch 2170] Loss: 3.1118\n",
      "[Batch 2180] Loss: 3.0470\n",
      "[Batch 2190] Loss: 1.1233\n",
      "[Batch 2200] Loss: 1.4097\n",
      "[Batch 2210] Loss: 1.9173\n",
      "[Batch 2220] Loss: 3.2429\n",
      "[Batch 2230] Loss: 2.1349\n",
      "[Batch 2240] Loss: 1.5823\n",
      "Epoch 9/10 | Loss: 4234.8722\n",
      "[Batch 0] Loss: 1.1096\n",
      "[Batch 10] Loss: 1.2469\n",
      "[Batch 20] Loss: 1.1756\n",
      "[Batch 30] Loss: 1.8447\n",
      "[Batch 40] Loss: 0.7264\n",
      "[Batch 50] Loss: 1.4720\n",
      "[Batch 60] Loss: 2.5972\n",
      "[Batch 70] Loss: 1.6117\n",
      "[Batch 80] Loss: 1.2625\n",
      "[Batch 90] Loss: 2.1414\n",
      "[Batch 100] Loss: 1.5428\n",
      "[Batch 110] Loss: 1.0316\n",
      "[Batch 120] Loss: 1.0035\n",
      "[Batch 130] Loss: 1.1435\n",
      "[Batch 140] Loss: 1.2607\n",
      "[Batch 150] Loss: 2.0576\n",
      "[Batch 160] Loss: 1.7551\n",
      "[Batch 170] Loss: 1.9172\n",
      "[Batch 180] Loss: 0.3539\n",
      "[Batch 190] Loss: 0.8252\n",
      "[Batch 200] Loss: 3.6245\n",
      "[Batch 210] Loss: 1.2367\n",
      "[Batch 220] Loss: 0.4331\n",
      "[Batch 230] Loss: 2.5997\n",
      "[Batch 240] Loss: 2.2493\n",
      "[Batch 250] Loss: 0.7751\n",
      "[Batch 260] Loss: 1.5735\n",
      "[Batch 270] Loss: 3.1766\n",
      "[Batch 280] Loss: 0.1392\n",
      "[Batch 290] Loss: 1.1755\n",
      "[Batch 300] Loss: 0.7316\n",
      "[Batch 310] Loss: 1.1398\n",
      "[Batch 320] Loss: 1.8257\n",
      "[Batch 330] Loss: 0.6739\n",
      "[Batch 340] Loss: 0.9406\n",
      "[Batch 350] Loss: 1.6725\n",
      "[Batch 360] Loss: 0.4339\n",
      "[Batch 370] Loss: 1.9244\n",
      "[Batch 380] Loss: 1.1020\n",
      "[Batch 390] Loss: 0.6831\n",
      "[Batch 400] Loss: 0.3030\n",
      "[Batch 410] Loss: 0.3436\n",
      "[Batch 420] Loss: 2.7570\n",
      "[Batch 430] Loss: 0.3907\n",
      "[Batch 440] Loss: 1.8488\n",
      "[Batch 450] Loss: 0.7119\n",
      "[Batch 460] Loss: 1.0966\n",
      "[Batch 470] Loss: 1.4729\n",
      "[Batch 480] Loss: 0.7066\n",
      "[Batch 490] Loss: 0.8768\n",
      "[Batch 500] Loss: 2.7781\n",
      "[Batch 510] Loss: 2.5317\n",
      "[Batch 520] Loss: 1.9704\n",
      "[Batch 530] Loss: 0.4793\n",
      "[Batch 540] Loss: 1.2583\n",
      "[Batch 550] Loss: 0.9594\n",
      "[Batch 560] Loss: 0.9396\n",
      "[Batch 570] Loss: 2.8190\n",
      "[Batch 580] Loss: 1.7111\n",
      "[Batch 590] Loss: 0.7704\n",
      "[Batch 600] Loss: 0.8199\n",
      "[Batch 610] Loss: 3.2306\n",
      "[Batch 620] Loss: 0.9358\n",
      "[Batch 630] Loss: 0.5409\n",
      "[Batch 640] Loss: 1.6195\n",
      "[Batch 650] Loss: 1.8186\n",
      "[Batch 660] Loss: 2.5109\n",
      "[Batch 670] Loss: 1.4261\n",
      "[Batch 680] Loss: 1.5794\n",
      "[Batch 690] Loss: 2.5658\n",
      "[Batch 700] Loss: 0.6404\n",
      "[Batch 710] Loss: 1.9905\n",
      "[Batch 720] Loss: 1.2027\n",
      "[Batch 730] Loss: 1.2865\n",
      "[Batch 740] Loss: 1.0936\n",
      "[Batch 750] Loss: 2.4910\n",
      "[Batch 760] Loss: 2.3828\n",
      "[Batch 770] Loss: 0.7406\n",
      "[Batch 780] Loss: 2.6467\n",
      "[Batch 790] Loss: 2.2588\n",
      "[Batch 800] Loss: 1.6715\n",
      "[Batch 810] Loss: 1.8318\n",
      "[Batch 820] Loss: 1.4542\n",
      "[Batch 830] Loss: 1.6067\n",
      "[Batch 840] Loss: 0.8437\n",
      "[Batch 850] Loss: 1.2751\n",
      "[Batch 860] Loss: 2.6255\n",
      "[Batch 870] Loss: 2.4553\n",
      "[Batch 880] Loss: 1.4147\n",
      "[Batch 890] Loss: 0.9716\n",
      "[Batch 900] Loss: 1.4915\n",
      "[Batch 910] Loss: 0.7768\n",
      "[Batch 920] Loss: 1.8469\n",
      "[Batch 930] Loss: 0.7875\n",
      "[Batch 940] Loss: 2.0243\n",
      "[Batch 950] Loss: 1.0369\n",
      "[Batch 960] Loss: 1.9312\n",
      "[Batch 970] Loss: 1.3982\n",
      "[Batch 980] Loss: 2.2898\n",
      "[Batch 990] Loss: 0.5019\n",
      "[Batch 1000] Loss: 1.1485\n",
      "[Batch 1010] Loss: 3.0112\n",
      "[Batch 1020] Loss: 0.6275\n",
      "[Batch 1030] Loss: 1.9837\n",
      "[Batch 1040] Loss: 1.6770\n",
      "[Batch 1050] Loss: 1.1098\n",
      "[Batch 1060] Loss: 0.9684\n",
      "[Batch 1070] Loss: 1.6039\n",
      "[Batch 1080] Loss: 2.0732\n",
      "[Batch 1090] Loss: 3.1096\n",
      "[Batch 1100] Loss: 1.2465\n",
      "[Batch 1110] Loss: 2.8311\n",
      "[Batch 1120] Loss: 1.7913\n",
      "[Batch 1130] Loss: 1.9946\n",
      "[Batch 1140] Loss: 0.9384\n",
      "[Batch 1150] Loss: 1.4623\n",
      "[Batch 1160] Loss: 2.8195\n",
      "[Batch 1170] Loss: 1.3290\n",
      "[Batch 1180] Loss: 1.9792\n",
      "[Batch 1190] Loss: 1.2390\n",
      "[Batch 1200] Loss: 1.2978\n",
      "[Batch 1210] Loss: 2.0386\n",
      "[Batch 1220] Loss: 2.8204\n",
      "[Batch 1230] Loss: 2.2590\n",
      "[Batch 1240] Loss: 0.8812\n",
      "[Batch 1250] Loss: 2.9377\n",
      "[Batch 1260] Loss: 1.7907\n",
      "[Batch 1270] Loss: 3.0723\n",
      "[Batch 1280] Loss: 1.3043\n",
      "[Batch 1290] Loss: 2.7344\n",
      "[Batch 1300] Loss: 1.8011\n",
      "[Batch 1310] Loss: 2.4318\n",
      "[Batch 1320] Loss: 1.6266\n",
      "[Batch 1330] Loss: 1.8944\n",
      "[Batch 1340] Loss: 1.4908\n",
      "[Batch 1350] Loss: 3.1606\n",
      "[Batch 1360] Loss: 2.2203\n",
      "[Batch 1370] Loss: 1.9023\n",
      "[Batch 1380] Loss: 1.3830\n",
      "[Batch 1390] Loss: 1.5124\n",
      "[Batch 1400] Loss: 1.7465\n",
      "[Batch 1410] Loss: 2.2412\n",
      "[Batch 1420] Loss: 0.9042\n",
      "[Batch 1430] Loss: 1.3113\n",
      "[Batch 1440] Loss: 1.8387\n",
      "[Batch 1450] Loss: 2.3876\n",
      "[Batch 1460] Loss: 3.2379\n",
      "[Batch 1470] Loss: 1.4826\n",
      "[Batch 1480] Loss: 1.4962\n",
      "[Batch 1490] Loss: 1.1383\n",
      "[Batch 1500] Loss: 3.4142\n",
      "[Batch 1510] Loss: 0.6903\n",
      "[Batch 1520] Loss: 1.6941\n",
      "[Batch 1530] Loss: 1.3138\n",
      "[Batch 1540] Loss: 1.0909\n",
      "[Batch 1550] Loss: 1.6184\n",
      "[Batch 1560] Loss: 2.6101\n",
      "[Batch 1570] Loss: 0.1757\n",
      "[Batch 1580] Loss: 1.2490\n",
      "[Batch 1590] Loss: 3.1067\n",
      "[Batch 1600] Loss: 2.0888\n",
      "[Batch 1610] Loss: 0.3385\n",
      "[Batch 1620] Loss: 1.5940\n",
      "[Batch 1630] Loss: 1.1900\n",
      "[Batch 1640] Loss: 1.3729\n",
      "[Batch 1650] Loss: 2.6726\n",
      "[Batch 1660] Loss: 3.5465\n",
      "[Batch 1670] Loss: 1.7758\n",
      "[Batch 1680] Loss: 0.2650\n",
      "[Batch 1690] Loss: 1.9728\n",
      "[Batch 1700] Loss: 0.7388\n",
      "[Batch 1710] Loss: 0.7583\n",
      "[Batch 1720] Loss: 1.6490\n",
      "[Batch 1730] Loss: 1.6190\n",
      "[Batch 1740] Loss: 3.2455\n",
      "[Batch 1750] Loss: 1.5602\n",
      "[Batch 1760] Loss: 1.0393\n",
      "[Batch 1770] Loss: 2.1573\n",
      "[Batch 1780] Loss: 1.8105\n",
      "[Batch 1790] Loss: 4.2659\n",
      "[Batch 1800] Loss: 2.8330\n",
      "[Batch 1810] Loss: 2.0045\n",
      "[Batch 1820] Loss: 0.8522\n",
      "[Batch 1830] Loss: 2.1166\n",
      "[Batch 1840] Loss: 1.1322\n",
      "[Batch 1850] Loss: 2.7367\n",
      "[Batch 1860] Loss: 2.0555\n",
      "[Batch 1870] Loss: 2.3974\n",
      "[Batch 1880] Loss: 2.2243\n",
      "[Batch 1890] Loss: 1.6178\n",
      "[Batch 1900] Loss: 1.9191\n",
      "[Batch 1910] Loss: 1.9827\n",
      "[Batch 1920] Loss: 0.7692\n",
      "[Batch 1930] Loss: 2.0942\n",
      "[Batch 1940] Loss: 1.5875\n",
      "[Batch 1950] Loss: 2.2611\n",
      "[Batch 1960] Loss: 2.0304\n",
      "[Batch 1970] Loss: 1.6944\n",
      "[Batch 1980] Loss: 1.4254\n",
      "[Batch 1990] Loss: 1.4773\n",
      "[Batch 2000] Loss: 2.5639\n",
      "[Batch 2010] Loss: 0.6671\n",
      "[Batch 2020] Loss: 0.3316\n",
      "[Batch 2030] Loss: 1.7129\n",
      "[Batch 2040] Loss: 0.6903\n",
      "[Batch 2050] Loss: 3.0089\n",
      "[Batch 2060] Loss: 1.7874\n",
      "[Batch 2070] Loss: 1.0199\n",
      "[Batch 2080] Loss: 1.9762\n",
      "[Batch 2090] Loss: 3.3381\n",
      "[Batch 2100] Loss: 2.0935\n",
      "[Batch 2110] Loss: 0.7685\n",
      "[Batch 2120] Loss: 0.5903\n",
      "[Batch 2130] Loss: 2.6896\n",
      "[Batch 2140] Loss: 0.6936\n",
      "[Batch 2150] Loss: 0.9701\n",
      "[Batch 2160] Loss: 0.9306\n",
      "[Batch 2170] Loss: 1.1652\n",
      "[Batch 2180] Loss: 1.0508\n",
      "[Batch 2190] Loss: 0.8674\n",
      "[Batch 2200] Loss: 1.1777\n",
      "[Batch 2210] Loss: 0.8075\n",
      "[Batch 2220] Loss: 0.4606\n",
      "[Batch 2230] Loss: 1.2282\n",
      "[Batch 2240] Loss: 2.3290\n",
      "Epoch 10/10 | Loss: 3756.4044\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.to(DEVICE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        try:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Batch {batch_idx}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Unexpected error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:58:59.994373Z",
     "iopub.status.busy": "2025-05-18T12:58:59.994073Z",
     "iopub.status.idle": "2025-05-18T12:59:19.459727Z",
     "shell.execute_reply": "2025-05-18T12:59:19.459101Z",
     "shell.execute_reply.started": "2025-05-18T12:58:59.994354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 38.60%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T13:00:57.768996Z",
     "iopub.status.busy": "2025-05-18T13:00:57.768700Z",
     "iopub.status.idle": "2025-05-18T13:00:57.773196Z",
     "shell.execute_reply": "2025-05-18T13:00:57.772416Z",
     "shell.execute_reply.started": "2025-05-18T13:00:57.768976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LORA_ADAPTER_DIR = \"/kaggle/working/vilt_vqa_lora_adapters\"\n",
    "\n",
    "os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T13:01:26.446095Z",
     "iopub.status.busy": "2025-05-18T13:01:26.445829Z",
     "iopub.status.idle": "2025-05-18T13:01:26.862012Z",
     "shell.execute_reply": "2025-05-18T13:01:26.861202Z",
     "shell.execute_reply.started": "2025-05-18T13:01:26.446077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured LoRA adapter directory exists: /kaggle/working/vilt_vqa_lora_adapters\n",
      "LoRA adapters (model weights and config) successfully saved to /kaggle/working/vilt_vqa_lora_adapters\n",
      "Processor configuration successfully saved to /kaggle/working/vilt_vqa_lora_adapters\n",
      "\n",
      "Contents of the adapter directory after saving:\n",
      "  - vocab.txt\n",
      "  - tokenizer_config.json\n",
      "  - README.md\n",
      "  - adapter_model.safetensors\n",
      "  - special_tokens_map.json\n",
      "  - preprocessor_config.json\n",
      "  - tokenizer.json\n",
      "  - adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "LORA_ADAPTER_DIR = \"/kaggle/working/vilt_vqa_lora_adapters\" # (from Cell 2)\n",
    "os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)\n",
    "print(f\"Ensured LoRA adapter directory exists: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(LORA_ADAPTER_DIR)\n",
    "    print(f\"LoRA adapters (model weights and config) successfully saved to {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "    processor.save_pretrained(LORA_ADAPTER_DIR)\n",
    "    print(f\"Processor configuration successfully saved to {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "    print(\"\\nContents of the adapter directory after saving:\")\n",
    "    for item in os.listdir(LORA_ADAPTER_DIR):\n",
    "        print(f\"  - {item}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during saving LoRA adapters or processor: {e}\")\n",
    "    print(\"Please check permissions, disk space, or the model/processor state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T13:01:41.694461Z",
     "iopub.status.busy": "2025-05-18T13:01:41.693847Z",
     "iopub.status.idle": "2025-05-18T13:01:43.215723Z",
     "shell.execute_reply": "2025-05-18T13:01:43.214973Z",
     "shell.execute_reply.started": "2025-05-18T13:01:41.694438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/vilt_vqa_lora_adapters/ (stored 0%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/vocab.txt (deflated 53%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/tokenizer_config.json (deflated 75%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/README.md (deflated 66%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/adapter_model.safetensors"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 8%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/special_tokens_map.json (deflated 80%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/preprocessor_config.json (deflated 51%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/tokenizer.json (deflated 71%)\n",
      "  adding: kaggle/working/vilt_vqa_lora_adapters/adapter_config.json (deflated 53%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/vilt_vqa_lora_adapters.zip /kaggle/working/vilt_vqa_lora_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7425078,
     "sourceId": 11820751,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7426888,
     "sourceId": 11823181,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
